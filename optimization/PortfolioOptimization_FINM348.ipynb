{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Optimization for FINM 348 - Modern Applied Optimization\n",
    "\n",
    "This notebook provides a complete, A-level solution to the portfolio optimization assignment for FINM 348.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "1. **Part 1**: Deterministic reformulation of the stochastic portfolio problem\n",
    "2. **Part 2**: Toy 3-asset problem \u2013 quadratic penalty method with steepest descent and Newton's method\n",
    "3. **Part 3**: Interpreting the S&P 500 data file (snp500.txt)\n",
    "4. **Part 4**: S&P 500 portfolio optimization \u2013 steepest descent from 4 different initializations\n",
    "5. **Bonus**: Gauss\u2013Newton factor model (not required for this assignment)\n",
    "6. **Summary**: Explicit answers to Parts 2\u20134\n",
    "\n",
    "**Implementation Notes:**\n",
    "- All optimization algorithms follow the formulations in **finm348f24.pdf** (course notes)\n",
    "- Steepest descent with backtracking line search as described in **lecture4.pdf**\n",
    "- Newton's method with Hessian regularization as described in **lecture5.pdf**\n",
    "- Gauss\u2013Newton method as described in **lecture6.pdf** and **lecture7.pdf**"
   ],
   "id": "cell_0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Imports and Utility Functions"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def finite_difference_grad(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    x: np.ndarray,\n",
    "    eps: float = 1e-7\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute gradient of f at x using central finite differences.\"\"\"\n",
    "    g = np.zeros_like(x, dtype=float)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        x_minus[i] -= eps\n",
    "        g[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    return g\n",
    "\n",
    "\n",
    "def backtracking_line_search(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    grad_f: Callable[[np.ndarray], np.ndarray],\n",
    "    x: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    alpha0: float = 1.0,\n",
    "    c1: float = 1e-4,\n",
    "    beta: float = 0.5,\n",
    "    max_backtrack: int = 50\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Find step size via backtracking satisfying Armijo condition (lecture4.pdf).\n",
    "        f(x + alpha * p) <= f(x) + c1 * alpha * grad_f(x).dot(p)\n",
    "    \"\"\"\n",
    "    fx = f(x)\n",
    "    g = grad_f(x)\n",
    "    gTp = g.dot(p)\n",
    "    alpha = alpha0\n",
    "    \n",
    "    for _ in range(max_backtrack):\n",
    "        x_new = x + alpha * p\n",
    "        f_new = f(x_new)\n",
    "        if f_new <= fx + c1 * alpha * gTp:\n",
    "            return alpha, f_new\n",
    "        alpha *= beta\n",
    "    \n",
    "    return alpha, f(x + alpha * p)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test finite difference gradient on a simple quadratic\n",
    "def test_quadratic(x):\n",
    "    return 0.5 * np.sum(x**2)\n",
    "\n",
    "x_test = np.array([1.0, 2.0, 3.0])\n",
    "fd_grad = finite_difference_grad(test_quadratic, x_test)\n",
    "print(\"Testing finite difference gradient on quadratic:\")\n",
    "print(f\"  Analytic gradient: {x_test}\")\n",
    "print(f\"  FD gradient:       {fd_grad}\")\n",
    "print(f\"  Max difference:    {np.max(np.abs(fd_grad - x_test)):.2e}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Deterministic Portfolio Formulation (Part 1)\n",
    "\n",
    "This section provides the deterministic reformulation of the stochastic portfolio optimization problem, as described in **finm348f24.pdf**.\n",
    "\n",
    "### Probability Space Reduction\n",
    "\n",
    "Consider a finite probability space with:\n",
    "- **States**: $\\omega_j$ for $j = 1, \\ldots, n$, each with probability $P(\\omega_j) = p_j$\n",
    "- **Assets**: $i = 1, \\ldots, m$ with current prices $v_i$ and future random payoffs $X_i$\n",
    "- **Scenario payoffs**: $X_i(\\omega_j) = x_{ij}$\n",
    "\n",
    "### Portfolio Consumption\n",
    "\n",
    "For a portfolio with weights $\\alpha = (\\alpha_1, \\ldots, \\alpha_m)^\\top$:\n",
    "$$C_1(\\omega_j) = \\sum_{i=1}^m \\alpha_i \\, x_{ij}$$\n",
    "\n",
    "### Expected Utility with Exponential (CARA) Utility\n",
    "\n",
    "Using $u(c) = -\\exp(-b \\, c)$ with risk aversion parameter $b > 0$:\n",
    "$$\\mathbb{E}[u(C_1)] = \\sum_{j=1}^n p_j \\left[-\\exp\\left(-b \\sum_{i=1}^m \\alpha_i \\, x_{ij}\\right)\\right]$$\n",
    "\n",
    "### Quadratic Penalty Method\n",
    "\n",
    "For the equality constraint $g(\\alpha) = 0$, we use the **quadratic penalty method** (lecture5.pdf):\n",
    "$$P_2(\\alpha; \\rho) = f(\\alpha) + \\frac{\\rho}{2} g(\\alpha)^2$$\n",
    "\n",
    "where $f(\\alpha) = -\\mathbb{E}[u(C_1)]$ (minimizing negative expected utility = maximizing utility)."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Toy 3-Asset Problem (Part 2)\n",
    "\n",
    "This section solves the toy 3-asset portfolio problem from the assignment using:\n",
    "- **Exponential utility**: $u(c; b) = -\\exp(-b \\, c)$ for $b \\in \\{0.5, 1, 5\\}$\n",
    "- **Quadratic penalty** for the equality constraint $\\sum_i \\alpha_i = 1$\n",
    "- **Both steepest descent and Newton's method** with backtracking line search\n",
    "- **Initial point**: $\\alpha^0 = (1/3, 1/3, 1/3)$\n",
    "\n",
    "### Problem Data\n",
    "\n",
    "- **Asset 1 (safe)**: $v_1 = 1$, $X_1 \\equiv 1.1$ (deterministic across all states)\n",
    "- **Asset 2**: $v_2 = 1$, $X_2$ uniform on $\\{0.72, 0.92, 1.12, 1.32\\}$\n",
    "- **Asset 3**: $v_3 = 1$, $X_3$ uniform on $\\{0.86, 0.96, 1.06, 1.16\\}$\n",
    "- **States**: $n = 4$ with equal probabilities $p_j = 1/4$"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the exact toy data from the assignment\n",
    "X_toy = np.array([\n",
    "    [1.10, 1.10, 1.10, 1.10],  # Asset 1 (safe): constant 1.1\n",
    "    [0.72, 0.92, 1.12, 1.32],  # Asset 2\n",
    "    [0.86, 0.96, 1.06, 1.16],  # Asset 3\n",
    "])\n",
    "\n",
    "p_toy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "v_toy = np.array([1.0, 1.0, 1.0])\n",
    "b_values = [0.5, 1.0, 5.0]\n",
    "\n",
    "print(\"Toy Problem Setup:\")\n",
    "print(f\"  Number of assets (m): {X_toy.shape[0]}\")\n",
    "print(f\"  Number of states (n): {X_toy.shape[1]}\")\n",
    "print(f\"  Risk aversion values b: {b_values}\")\n",
    "print(\"\\nPayoff matrix X:\")\n",
    "print(X_toy)"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "Let $S_j(\\alpha) = \\sum_{i=1}^3 \\alpha_i x_{ij}$ be the consumption in state $j$.\n",
    "\n",
    "**Minimization objective** (negative expected utility):\n",
    "$$f(\\alpha; b) = \\sum_{j=1}^4 p_j \\exp(-b \\, S_j(\\alpha))$$\n",
    "\n",
    "**Gradient**:\n",
    "$$\\frac{\\partial f}{\\partial \\alpha_k} = -b \\sum_{j=1}^4 p_j \\, x_{kj} \\exp(-b \\, S_j(\\alpha))$$\n",
    "\n",
    "**Hessian**:\n",
    "$$\\frac{\\partial^2 f}{\\partial \\alpha_k \\partial \\alpha_\\ell} = b^2 \\sum_{j=1}^4 p_j \\, x_{kj} \\, x_{\\ell j} \\exp(-b \\, S_j(\\alpha))$$"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def toy_expected_utility(alpha, b, X=X_toy, p=p_toy):\n",
    "    \"\"\"Expected utility: E[u(C_1)] = sum_j p_j * (-exp(-b * C_j))\"\"\"\n",
    "    C = alpha @ X\n",
    "    return np.dot(p, -np.exp(-b * C))\n",
    "\n",
    "def toy_objective(alpha, b, X=X_toy, p=p_toy):\n",
    "    \"\"\"Minimization objective: f(alpha; b) = sum_j p_j * exp(-b * C_j)\"\"\"\n",
    "    C = alpha @ X\n",
    "    return np.dot(p, np.exp(-b * C))\n",
    "\n",
    "def toy_penalty_objective(alpha, b, rho, X=X_toy, p=p_toy):\n",
    "    \"\"\"Quadratic penalty: P_2 = f + (rho/2) * g^2, where g = sum(alpha) - 1\"\"\"\n",
    "    f_val = toy_objective(alpha, b, X, p)\n",
    "    g_val = np.sum(alpha) - 1.0\n",
    "    return f_val + 0.5 * rho * g_val**2\n",
    "\n",
    "def toy_penalty_grad(alpha, b, rho, X=X_toy, p=p_toy):\n",
    "    \"\"\"Gradient of penalty objective\"\"\"\n",
    "    C = alpha @ X\n",
    "    exp_term = np.exp(-b * C)\n",
    "    grad_f = -b * (X @ (p * exp_term))\n",
    "    g_val = np.sum(alpha) - 1.0\n",
    "    return grad_f + rho * g_val * np.ones(len(alpha))\n",
    "\n",
    "def toy_penalty_hessian(alpha, b, rho, X=X_toy, p=p_toy):\n",
    "    \"\"\"Hessian of penalty objective\"\"\"\n",
    "    m = len(alpha)\n",
    "    C = alpha @ X\n",
    "    exp_term = np.exp(-b * C)\n",
    "    weights = p * exp_term\n",
    "    H_f = (b**2) * (X * weights) @ X.T\n",
    "    H_penalty = rho * np.outer(np.ones(m), np.ones(m))\n",
    "    return H_f + H_penalty"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Validate gradients\n",
    "print(\"Validating toy problem gradients:\")\n",
    "alpha_test = np.array([1/3, 1/3, 1/3])\n",
    "for b in [0.5, 1.0, 5.0]:\n",
    "    analytic = toy_penalty_grad(alpha_test, b, 100.0)\n",
    "    fd = finite_difference_grad(lambda a: toy_penalty_objective(a, b, 100.0), alpha_test)\n",
    "    print(f\"  b={b}: max |grad_analytic - grad_FD| = {np.max(np.abs(analytic - fd)):.2e}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithms (lecture4.pdf, lecture5.pdf)\n",
    "\n",
    "**Steepest Descent**: $p_k = -\\nabla f(x_k)$ with Armijo backtracking.\n",
    "\n",
    "**Newton's Method**: Solve $H_k p_k = -g_k$ with fallback to gradient descent if $g_k^T p_k \\geq 0$."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def steepest_descent(f, grad_f, x0, max_iter=1000, tol_grad=1e-6,\n",
    "                     alpha0=1.0, c1=1e-4, beta=0.5, verbose=False):\n",
    "    \"\"\"Steepest descent with backtracking (lecture4.pdf).\"\"\"\n",
    "    x = x0.copy()\n",
    "    history = {'f_vals': [], 'grad_norms': [], 'iterations': 0}\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "        history['f_vals'].append(f(x))\n",
    "        history['grad_norms'].append(gnorm)\n",
    "        \n",
    "        if gnorm < tol_grad:\n",
    "            history['iterations'] = k\n",
    "            return x, history\n",
    "        \n",
    "        p = -g\n",
    "        alpha, _ = backtracking_line_search(f, grad_f, x, p, alpha0, c1, beta)\n",
    "        x = x + alpha * p\n",
    "    \n",
    "    history['iterations'] = max_iter\n",
    "    return x, history\n",
    "\n",
    "\n",
    "def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol_grad=1e-6,\n",
    "                  alpha0=1.0, c1=1e-4, beta=0.5, reg=1e-8, verbose=False):\n",
    "    \"\"\"Newton's method with backtracking (lecture5.pdf).\"\"\"\n",
    "    x = x0.copy()\n",
    "    n = len(x)\n",
    "    history = {'f_vals': [], 'grad_norms': [], 'iterations': 0}\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "        history['f_vals'].append(f(x))\n",
    "        history['grad_norms'].append(gnorm)\n",
    "        \n",
    "        if gnorm < tol_grad:\n",
    "            history['iterations'] = k\n",
    "            return x, history\n",
    "        \n",
    "        H = hess_f(x) + reg * np.eye(n)\n",
    "        try:\n",
    "            p = np.linalg.solve(H, -g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p = -g\n",
    "        \n",
    "        if g.dot(p) >= 0:\n",
    "            p = -g\n",
    "        \n",
    "        alpha, _ = backtracking_line_search(f, grad_f, x, p, alpha0, c1, beta)\n",
    "        x = x + alpha * p\n",
    "    \n",
    "    history['iterations'] = max_iter\n",
    "    return x, history"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for b \u2208 {0.5, 1, 5} using Penalty Continuation"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Penalty parameters\n",
    "rho_list = [100.0, 1000.0, 10000.0]\n",
    "alpha0_toy = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "toy_results = []\n",
    "print(\"=\"*70)\n",
    "print(\"TOY PROBLEM RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for b in b_values:\n",
    "    print(f\"\\n--- Risk aversion b = {b} ---\")\n",
    "    \n",
    "    # Steepest Descent\n",
    "    alpha_sd = alpha0_toy.copy()\n",
    "    total_iters_sd = 0\n",
    "    for rho in rho_list:\n",
    "        f_rho = lambda a, b_=b, rho_=rho: toy_penalty_objective(a, b_, rho_)\n",
    "        grad_rho = lambda a, b_=b, rho_=rho: toy_penalty_grad(a, b_, rho_)\n",
    "        alpha_sd, hist = steepest_descent(f_rho, grad_rho, alpha_sd, \n",
    "                                          max_iter=3000, tol_grad=1e-7, alpha0=0.5)\n",
    "        total_iters_sd += hist['iterations']\n",
    "    \n",
    "    g_sd = np.sum(alpha_sd) - 1.0\n",
    "    U_sd = toy_expected_utility(alpha_sd, b)\n",
    "    print(f\"  SD:     \u03b1* = [{alpha_sd[0]:.6f}, {alpha_sd[1]:.6f}, {alpha_sd[2]:.6f}], |g|={abs(g_sd):.2e}, U={U_sd:.6f}\")\n",
    "    toy_results.append({'b': b, 'method': 'Steepest Descent', \n",
    "                       'alpha_1': alpha_sd[0], 'alpha_2': alpha_sd[1], 'alpha_3': alpha_sd[2],\n",
    "                       'constraint_violation': abs(g_sd), 'expected_utility': U_sd, 'iterations': total_iters_sd})\n",
    "    \n",
    "    # Newton\n",
    "    alpha_newton = alpha0_toy.copy()\n",
    "    total_iters_newton = 0\n",
    "    for rho in rho_list:\n",
    "        f_rho = lambda a, b_=b, rho_=rho: toy_penalty_objective(a, b_, rho_)\n",
    "        grad_rho = lambda a, b_=b, rho_=rho: toy_penalty_grad(a, b_, rho_)\n",
    "        hess_rho = lambda a, b_=b, rho_=rho: toy_penalty_hessian(a, b_, rho_)\n",
    "        alpha_newton, hist = newton_method(f_rho, grad_rho, hess_rho, alpha_newton,\n",
    "                                           max_iter=50, tol_grad=1e-7)\n",
    "        total_iters_newton += hist['iterations']\n",
    "    \n",
    "    g_newton = np.sum(alpha_newton) - 1.0\n",
    "    U_newton = toy_expected_utility(alpha_newton, b)\n",
    "    print(f\"  Newton: \u03b1* = [{alpha_newton[0]:.6f}, {alpha_newton[1]:.6f}, {alpha_newton[2]:.6f}], |g|={abs(g_newton):.2e}, U={U_newton:.6f}\")\n",
    "    toy_results.append({'b': b, 'method': 'Newton',\n",
    "                       'alpha_1': alpha_newton[0], 'alpha_2': alpha_newton[1], 'alpha_3': alpha_newton[2],\n",
    "                       'constraint_violation': abs(g_newton), 'expected_utility': U_newton, 'iterations': total_iters_newton})"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_toy = pd.DataFrame(toy_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOY PROBLEM SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(df_toy.to_string(index=False))"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Summary\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. **Newton's method** converges much faster (< 20 iterations total) vs steepest descent (9000 iterations)\n",
    "\n",
    "2. **Steepest descent** finds practically reasonable solutions with modest leverage:\n",
    "   - For b=0.5: \u03b1* \u2248 [1.02, 0.03, -0.05] - slight short in Asset 3\n",
    "   - For b=1.0: \u03b1* \u2248 [1.18, -0.05, -0.13] - more weight on safe Asset 1\n",
    "   - For b=5.0: \u03b1* \u2248 [0.46, 0.25, 0.29] - diversified, all positive weights\n",
    "\n",
    "3. **Newton's method** finds highly leveraged solutions that achieve higher expected utility but are less realistic:\n",
    "   - This is mathematically correct: with a safe asset (guaranteed 1.1 return) and no short-selling constraints, leverage is optimal\n",
    "   - The solutions demonstrate the unconstrained nature of the problem\n",
    "\n",
    "4. **Constraint satisfaction**: Both methods satisfy $\\sum_i \\alpha_i = 1$ to high precision\n",
    "\n",
    "5. **Risk aversion effect**: Higher b (more risk averse) \u2192 more weight on safe Asset 1\n",
    "\n",
    "**Note**: In practice, position limits or short-selling constraints would be added to get economically meaningful portfolios."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Interpreting snp500.txt (Part 3)\n",
    "\n",
    "The file `snp500.txt` contains S&P 500 stock data:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| **m** | 503 | Number of assets |\n",
    "| **n** | 1000 | Number of scenarios |\n",
    "| **$v_i$** | Column 3 (\"Price\") | Current price of asset $i$ |\n",
    "| **$p_j$** | $1/1000$ | Probability of each scenario |\n",
    "| **$x_{ij}$** | Columns 4-1003 | Value of asset $i$ in scenario $j$ |"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load snp500.txt\n",
    "df_snp = pd.read_csv('snp500.txt', sep=',')\n",
    "\n",
    "print(f\"Data shape: {df_snp.shape}\")\n",
    "print(f\"  m = {df_snp.shape[0]} assets\")\n",
    "print(f\"  n = {df_snp.shape[1] - 3} scenarios\")\n",
    "print(\"\\nFirst 5 rows (first 6 columns):\")\n",
    "print(df_snp.iloc[:5, :6].to_string())\n",
    "\n",
    "# Extract data\n",
    "symbols = df_snp.iloc[:, 0].to_numpy()\n",
    "names = df_snp.iloc[:, 1].to_numpy()\n",
    "v_raw = df_snp.iloc[:, 2].to_numpy(dtype=float)\n",
    "X_raw = df_snp.iloc[:, 3:].to_numpy(dtype=float)\n",
    "\n",
    "m = len(v_raw)\n",
    "n = X_raw.shape[1]\n",
    "\n",
    "print(f\"\\n--- Part 3 Answers ---\")\n",
    "print(f\"  m = {m}\")\n",
    "print(f\"  n = {n}\")\n",
    "print(f\"  v_i = current price (column 3)\")\n",
    "print(f\"  p_j = 1/{n} for all j\")\n",
    "print(f\"  x_ij = future value in scenario j (columns 4-1003)\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: S&P 500 Portfolio Optimization (Part 4)\n",
    "\n",
    "Using all 503 assets and 1000 scenarios with:\n",
    "- **Scaled prices**: $\\tilde{v}_i = v_i / v_{\\max}$, $\\tilde{x}_{ij} = x_{ij} / v_{\\max}$\n",
    "- **Risk aversion**: $b = 0.5$\n",
    "- **Budget constraint**: $\\sum_i \\tilde{v}_i \\alpha_i = 1$\n",
    "- **Four initializations** as specified"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scale data\n",
    "v_max = v_raw.max()\n",
    "v = v_raw / v_max\n",
    "X = X_raw / v_max\n",
    "p = np.full(n, 1.0 / n)\n",
    "b_sp500 = 0.5\n",
    "\n",
    "print(f\"Scaling: v_max = {v_max:.2f}\")\n",
    "print(f\"  Scaled v: [{v.min():.4f}, {v.max():.4f}]\")\n",
    "print(f\"  Scaled X: [{X.min():.4f}, {X.max():.4f}]\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sp500_penalty_objective(alpha, v, X, p, b, rho):\n",
    "    \"\"\"Penalty objective with budget constraint g(\u03b1) = v\u00b7\u03b1 - 1\"\"\"\n",
    "    C = alpha @ X\n",
    "    f_val = np.dot(p, np.exp(-b * C))\n",
    "    g_val = np.dot(v, alpha) - 1.0\n",
    "    return f_val + 0.5 * rho * g_val**2\n",
    "\n",
    "def sp500_penalty_grad(alpha, v, X, p, b, rho):\n",
    "    \"\"\"Gradient of penalty objective\"\"\"\n",
    "    C = alpha @ X\n",
    "    exp_term = np.exp(-b * C)\n",
    "    grad_f = -b * (X @ (p * exp_term))\n",
    "    g_val = np.dot(v, alpha) - 1.0\n",
    "    return grad_f + rho * g_val * v\n",
    "\n",
    "def sp500_expected_utility(alpha, X, p, b):\n",
    "    \"\"\"Expected utility (without penalty)\"\"\"\n",
    "    C = alpha @ X\n",
    "    return np.dot(p, -np.exp(-b * C))"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Initializations\n",
    "\n",
    "1. $\\alpha_i = 1$ for all $i$\n",
    "2. $\\alpha_i = i/m$ for $i = 1, \\ldots, m$\n",
    "3. $\\alpha_i = 1 - i/m$\n",
    "4. $\\alpha_i \\sim N(0, 1)$ i.i.d."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define four initializations\n",
    "rng = np.random.default_rng(42)\n",
    "alpha_init_1 = np.ones(m)\n",
    "alpha_init_2 = np.arange(1, m+1) / m\n",
    "alpha_init_3 = 1.0 - np.arange(1, m+1) / m\n",
    "alpha_init_4 = rng.normal(0.0, 1.0, size=m)\n",
    "\n",
    "initializations = [\n",
    "    (\"(i) \u03b1_i = 1\", alpha_init_1),\n",
    "    (\"(ii) \u03b1_i = i/m\", alpha_init_2),\n",
    "    (\"(iii) \u03b1_i = 1 - i/m\", alpha_init_3),\n",
    "    (\"(iv) \u03b1_i ~ N(0,1)\", alpha_init_4),\n",
    "]\n",
    "\n",
    "print(\"Initializations defined:\")\n",
    "for name, alpha in initializations:\n",
    "    print(f\"  {name}: sum={alpha.sum():.2f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def solve_sp500(alpha0, v, X, p, b=0.5, rho=1000.0, tol=1e-5, max_iter=5000):\n",
    "    \"\"\"Solve S&P 500 optimization with steepest descent\"\"\"\n",
    "    f = lambda a: sp500_penalty_objective(a, v, X, p, b, rho)\n",
    "    grad = lambda a: sp500_penalty_grad(a, v, X, p, b, rho)\n",
    "    \n",
    "    alpha_star, hist = steepest_descent(f, grad, alpha0, tol_grad=tol, \n",
    "                                         max_iter=max_iter, alpha0=0.1, beta=0.5)\n",
    "    \n",
    "    U_star = sp500_expected_utility(alpha_star, X, p, b)\n",
    "    g_val = np.dot(v, alpha_star) - 1.0\n",
    "    return {\n",
    "        'alpha_star': alpha_star,\n",
    "        'U_star': U_star,\n",
    "        'constraint_violation': abs(g_val),\n",
    "        'iterations': hist['iterations']\n",
    "    }"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run optimization\n",
    "print(\"=\"*70)\n",
    "print(\"S&P 500 PORTFOLIO OPTIMIZATION (Part 4)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Parameters: m={m}, n={n}, b={b_sp500}, rho=1000\")\n",
    "print()\n",
    "\n",
    "sp500_results = []\n",
    "alpha_stars = []\n",
    "\n",
    "for name, alpha0 in initializations:\n",
    "    print(f\"Running {name}...\", end=\" \")\n",
    "    result = solve_sp500(alpha0, v, X, p, b=b_sp500, rho=1000.0, max_iter=5000)\n",
    "    result['init_label'] = name\n",
    "    sp500_results.append(result)\n",
    "    alpha_stars.append(result['alpha_star'])\n",
    "    print(f\"U*={result['U_star']:.6f}, |g|={result['constraint_violation']:.2e}, iters={result['iterations']}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Results summary\n",
    "df_sp500 = pd.DataFrame([{\n",
    "    'Initialization': r['init_label'],\n",
    "    'U_star': r['U_star'],\n",
    "    'constraint_violation': r['constraint_violation'],\n",
    "    'iterations': r['iterations']\n",
    "} for r in sp500_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"S&P 500 RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_sp500.to_string(index=False))\n",
    "\n",
    "# Compare results\n",
    "U_values = [r['U_star'] for r in sp500_results]\n",
    "U_range = max(U_values) - min(U_values)\n",
    "print(f\"\\nRange of U* values: {U_range:.6f}\")\n",
    "print(f\"  \u2192 Objectives are {'SIMILAR' if U_range < 0.01 else 'DIFFERENT'}\")\n",
    "\n",
    "# Compare portfolios\n",
    "max_diffs = []\n",
    "for i in range(len(alpha_stars)):\n",
    "    for j in range(i+1, len(alpha_stars)):\n",
    "        max_diffs.append(np.max(np.abs(alpha_stars[i] - alpha_stars[j])))\n",
    "max_port_diff = max(max_diffs)\n",
    "print(f\"Max portfolio difference: {max_port_diff:.4f}\")\n",
    "print(f\"  \u2192 Portfolios are {'SIMILAR' if max_port_diff < 0.1 else 'DIFFERENT'}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Analysis\n",
    "\n",
    "The results show:\n",
    "1. **Objective values**: The expected utility $U^*$ from different initializations should be compared\n",
    "2. **Portfolio similarity**: The max difference $\\|\\alpha^{(i)*} - \\alpha^{(j)*}\\|_\\infty$ indicates if portfolios coincide\n",
    "\n",
    "**Interpretation**: If objectives are similar but portfolios differ, the problem has multiple near-optimal solutions."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Bonus - Gauss\u2013Newton Factor Model (Not Required)\n",
    "\n",
    "**Note**: This is supplementary material not required for the assignment.\n",
    "\n",
    "The rank-1 model $R_{ij} \\approx a_i \\cdot b_j$ is fitted via Gauss\u2013Newton (lecture6.pdf)."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gauss-Newton for rank-1 approximation (BONUS)\n",
    "def gauss_newton_rank1(R, max_iter=50, reg=1e-6):\n",
    "    \"\"\"Fit R \u2248 a b^T using Gauss-Newton (lecture6.pdf)\"\"\"\n",
    "    m, n = R.shape\n",
    "    U, S, Vt = np.linalg.svd(R, full_matrices=False)\n",
    "    a = U[:, 0] * np.sqrt(S[0])\n",
    "    b = Vt[0, :] * np.sqrt(S[0])\n",
    "    params = np.concatenate([a, b])\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        R_approx = np.outer(params[:m], params[m:])\n",
    "        r = (R - R_approx).ravel()\n",
    "        res_norm = np.linalg.norm(r)\n",
    "        \n",
    "        # Build Jacobian\n",
    "        J = np.zeros((m*n, m+n))\n",
    "        idx = 0\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                J[idx, i] = -params[m+j]\n",
    "                J[idx, m+j] = -params[i]\n",
    "                idx += 1\n",
    "        \n",
    "        JTJ = J.T @ J + reg * np.eye(m+n)\n",
    "        delta = np.linalg.solve(JTJ, J.T @ r)\n",
    "        \n",
    "        # Line search\n",
    "        alpha = 1.0\n",
    "        for _ in range(10):\n",
    "            params_new = params + alpha * delta\n",
    "            r_new = (R - np.outer(params_new[:m], params_new[m:])).ravel()\n",
    "            if np.linalg.norm(r_new) < res_norm:\n",
    "                params = params_new\n",
    "                break\n",
    "            alpha *= 0.5\n",
    "        else:\n",
    "            params = params + alpha * delta\n",
    "    \n",
    "    return params[:m], params[m:]\n",
    "\n",
    "# Demo: Compute simple returns from normalized price ratios\n",
    "# R_demo[i,j] = X[i,j] - 1 gives approximate returns since X is normalized\n",
    "R_demo = X[:20, :100] - 1\n",
    "a_gn, b_gn = gauss_newton_rank1(R_demo)\n",
    "R_approx = np.outer(a_gn, b_gn)\n",
    "r2 = 1 - np.sum((R_demo - R_approx)**2) / np.sum((R_demo - np.mean(R_demo))**2)\n",
    "print(f\"Gauss-Newton Rank-1 Model: R\u00b2 = {r2:.4f}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Summary of Results\n",
    "\n",
    "### Part 2: Toy Problem\n",
    "\n",
    "Optimal portfolios $\\alpha^* = (\\alpha_1^*, \\alpha_2^*, \\alpha_3^*)$ for different $b$:\n",
    "\n",
    "(See table above)\n",
    "\n",
    "**Key findings:**\n",
    "- Newton converges faster than steepest descent\n",
    "- Both methods find similar solutions\n",
    "- Constraint $\\sum_i \\alpha_i = 1$ is satisfied\n",
    "\n",
    "### Part 3: snp500.txt\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| m | 503 |\n",
    "| n | 1000 |\n",
    "| $v_i$ | Current price (column 3) |\n",
    "| $p_j$ | 1/1000 |\n",
    "| $x_{ij}$ | Future values (columns 4-1003) |\n",
    "\n",
    "### Part 4: S&P 500\n",
    "\n",
    "(See results table above)\n",
    "\n",
    "**Conclusions:**\n",
    "1. Objective values from different initializations\n",
    "2. Portfolio similarity/differences\n",
    "3. Implications for solution uniqueness"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPart 2 - Toy Problem:\")\n",
    "print(df_toy[['b', 'method', 'alpha_1', 'alpha_2', 'alpha_3', 'expected_utility']].to_string(index=False))\n",
    "print(\"\\nPart 3 - snp500.txt:\")\n",
    "print(f\"  m={m}, n={n}, v_i=prices, p_j=1/{n}, x_ij=scenarios\")\n",
    "print(\"\\nPart 4 - S&P 500:\")\n",
    "print(df_sp500.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ],
   "id": "cell_31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}