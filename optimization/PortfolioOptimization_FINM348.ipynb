{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_0",
   "metadata": {},
   "source": [
    "# Portfolio Optimization for FINM 348\n",
    "\n",
    "This notebook covers portfolio optimization using continuous optimization methods including:\n",
    "\n",
    "1. Deterministic portfolio optimization formulation\n",
    "2. A toy 3-asset portfolio problem using quadratic penalty for equality constraints\n",
    "3. Implementations of steepest descent and Newton's method with backtracking line search\n",
    "4. Reading and scaling S&P 500 data\n",
    "5. Portfolio optimization on S&P 500 using steepest descent from multiple initializations\n",
    "6. A \"week-only\" return model estimated via Gauss\u2013Newton\n",
    "7. Numerical experiments and diagnostics\n",
    "\n",
    "## Plan and Test Strategy\n",
    "\n",
    "- Test gradient implementations via finite difference comparisons\n",
    "- Verify steepest descent and Newton on small toy problems before scaling\n",
    "- Use small subsets of S&P 500 data (e.g., first 20 assets, 100 scenarios) for speed\n",
    "- Validate constraint satisfaction at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_1",
   "metadata": {},
   "source": [
    "## Section 0: Imports and Helper Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, Optional, Tuple, List\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_grad(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    x: np.ndarray,\n",
    "    eps: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute gradient of f at x using forward finite differences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x) -> float\n",
    "    x : np.ndarray\n",
    "        Point at which to evaluate the gradient\n",
    "    eps : float\n",
    "        Perturbation size for finite differences\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Approximate gradient of f at x\n",
    "    \"\"\"\n",
    "    g = np.zeros_like(x, dtype=float)\n",
    "    fx = f(x)\n",
    "    for i in range(len(x)):\n",
    "        x_pert = x.copy()\n",
    "        x_pert[i] += eps\n",
    "        g[i] = (f(x_pert) - fx) / eps\n",
    "    return g\n",
    "\n",
    "\n",
    "def backtracking_line_search(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    grad_f: Callable[[np.ndarray], np.ndarray],\n",
    "    x: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    alpha0: float = 1.0,\n",
    "    c1: float = 1e-4,\n",
    "    beta: float = 0.5,\n",
    "    max_backtrack: int = 30\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Find step size alpha > 0 via backtracking satisfying Armijo condition:\n",
    "        f(x + alpha * p) <= f(x) + c1 * alpha * grad_f(x).dot(p)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function\n",
    "    grad_f : callable\n",
    "        Gradient of objective function\n",
    "    x : np.ndarray\n",
    "        Current point\n",
    "    p : np.ndarray\n",
    "        Search direction\n",
    "    alpha0 : float\n",
    "        Initial step size\n",
    "    c1 : float\n",
    "        Armijo constant (typically 1e-4)\n",
    "    beta : float\n",
    "        Backtracking reduction factor (typically 0.5)\n",
    "    max_backtrack : int\n",
    "        Maximum number of backtracking iterations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple (alpha, f_new)\n",
    "        The step size and new function value\n",
    "    \"\"\"\n",
    "    fx = f(x)\n",
    "    g = grad_f(x)\n",
    "    gTp = g.dot(p)\n",
    "    alpha = alpha0\n",
    "    \n",
    "    for _ in range(max_backtrack):\n",
    "        x_new = x + alpha * p\n",
    "        f_new = f(x_new)\n",
    "        if f_new <= fx + c1 * alpha * gTp:\n",
    "            return alpha, f_new\n",
    "        alpha *= beta\n",
    "    \n",
    "    # Return whatever we have after max iterations\n",
    "    x_new = x + alpha * p\n",
    "    f_new = f(x_new)\n",
    "    return alpha, f_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test finite difference gradient on a simple quadratic\n",
    "def test_quadratic(x):\n",
    "    return 0.5 * np.sum(x**2)\n",
    "\n",
    "def test_quadratic_grad(x):\n",
    "    return x.copy()\n",
    "\n",
    "x_test = np.array([1.0, 2.0, 3.0])\n",
    "fd_grad = finite_difference_grad(test_quadratic, x_test)\n",
    "analytic_grad = test_quadratic_grad(x_test)\n",
    "print(\"Testing finite difference gradient:\")\n",
    "print(f\"  Analytic gradient: {analytic_grad}\")\n",
    "print(f\"  FD gradient:       {fd_grad}\")\n",
    "print(f\"  Max difference:    {np.max(np.abs(fd_grad - analytic_grad)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_5",
   "metadata": {},
   "source": [
    "## Section 1: Deterministic Portfolio Formulation\n",
    "\n",
    "Consider a portfolio optimization problem with:\n",
    "\n",
    "- **States** $j = 1, \\dots, n$ with probabilities $p_j$ (typically $p_j = 1/n$ for empirical distribution)\n",
    "- **Assets** $i = 1, \\dots, m$ with scenario payoffs $x_{ij}$ representing the return of asset $i$ in state $j$\n",
    "- **Portfolio weights** $\\alpha \\in \\mathbb{R}^m$ representing the fraction invested in each asset\n",
    "\n",
    "The portfolio consumption in state $j$ is:\n",
    "$$C_1(\\omega_j) = \\sum_{i=1}^m \\alpha_i x_{ij}$$\n",
    "\n",
    "### Objective: Expected Utility Maximization\n",
    "\n",
    "For exponential (CARA) utility with risk aversion $\\gamma > 0$:\n",
    "$$u(c) = -\\exp(-\\gamma c)$$\n",
    "\n",
    "The expected utility is:\n",
    "$$\\mathbb{E}[u(C_1)] = \\sum_{j=1}^n p_j \\, u\\left(\\sum_{i=1}^m \\alpha_i x_{ij}\\right)$$\n",
    "\n",
    "### Constraint: Budget Constraint\n",
    "\n",
    "We impose that the portfolio weights sum to one:\n",
    "$$\\mathbf{e}^\\top \\alpha = 1$$\n",
    "\n",
    "where $\\mathbf{e} = (1, 1, \\dots, 1)^\\top$.\n",
    "\n",
    "### Quadratic Penalty Formulation\n",
    "\n",
    "To handle the equality constraint, we use a quadratic penalty method. The penalty objective is:\n",
    "$$P(\\alpha, \\rho) = f(\\alpha) + \\frac{\\rho}{2}(\\mathbf{e}^\\top \\alpha - 1)^2$$\n",
    "\n",
    "where $f(\\alpha) = -\\mathbb{E}[u(C_1)]$ (we minimize the negative expected utility, i.e., maximize expected utility).\n",
    "\n",
    "As $\\rho \\to \\infty$, minimizers of $P(\\alpha, \\rho)$ approach the solution of the constrained problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_6",
   "metadata": {},
   "source": [
    "## Section 2: Toy Three-Asset Problem with Quadratic Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small 3-asset payoff matrix (3 assets \u00d7 5 states)\n",
    "# Each column represents returns in a given state\n",
    "np.random.seed(42)\n",
    "\n",
    "m_toy = 3  # number of assets\n",
    "n_toy = 5  # number of states\n",
    "\n",
    "# Payoff matrix: returns for each asset in each state\n",
    "# Rows = assets, Columns = states\n",
    "X_toy = np.array([\n",
    "    [1.05, 0.98, 1.02, 0.95, 1.08],  # Asset 0: moderate risk\n",
    "    [1.10, 0.85, 1.15, 0.90, 1.20],  # Asset 1: high risk\n",
    "    [1.01, 1.01, 1.01, 1.01, 1.01],  # Asset 2: risk-free-like\n",
    "])\n",
    "\n",
    "# Equal probabilities for each state\n",
    "p_toy = np.ones(n_toy) / n_toy\n",
    "\n",
    "# Risk aversion parameter for exponential utility\n",
    "gamma_toy = 2.0\n",
    "\n",
    "print(f\"Toy problem: {m_toy} assets, {n_toy} states\")\n",
    "print(f\"Payoff matrix X shape: {X_toy.shape}\")\n",
    "print(f\"Probabilities: {p_toy}\")\n",
    "print(f\"Risk aversion \u03b3 = {gamma_toy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_utility(c: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Exponential (CARA) utility: u(c) = -exp(-\u03b3c)\"\"\"\n",
    "    return -np.exp(-gamma * c)\n",
    "\n",
    "\n",
    "def toy_objective(alpha: np.ndarray, X: np.ndarray = X_toy, \n",
    "                  p: np.ndarray = p_toy, gamma: float = gamma_toy) -> float:\n",
    "    \"\"\"\n",
    "    Negative expected utility (to be minimized).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : np.ndarray\n",
    "        Portfolio weights (m,)\n",
    "    X : np.ndarray\n",
    "        Payoff matrix (m \u00d7 n)\n",
    "    p : np.ndarray\n",
    "        State probabilities (n,)\n",
    "    gamma : float\n",
    "        Risk aversion parameter\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Negative expected utility\n",
    "    \"\"\"\n",
    "    # Portfolio consumption in each state: C_j = sum_i alpha_i * X_ij\n",
    "    consumption = X.T @ alpha  # shape (n,)\n",
    "    utilities = exp_utility(consumption, gamma)\n",
    "    expected_utility = p @ utilities\n",
    "    return -expected_utility  # minimize negative = maximize\n",
    "\n",
    "\n",
    "def toy_objective_grad(alpha: np.ndarray, X: np.ndarray = X_toy,\n",
    "                       p: np.ndarray = p_toy, gamma: float = gamma_toy) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient of negative expected utility.\n",
    "    \n",
    "    For u(c) = -exp(-\u03b3c), we have u'(c) = \u03b3 exp(-\u03b3c).\n",
    "    \n",
    "    \u2202f/\u2202\u03b1_i = -\u2211_j p_j * u'(C_j) * X_ij\n",
    "            = -\u2211_j p_j * \u03b3 * exp(-\u03b3 C_j) * X_ij\n",
    "    \"\"\"\n",
    "    consumption = X.T @ alpha  # (n,)\n",
    "    u_prime = gamma * np.exp(-gamma * consumption)  # (n,)\n",
    "    # Chain rule: multiply by X_ij and sum over states\n",
    "    grad = -X @ (p * u_prime)  # (m,)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient with finite differences\n",
    "alpha_test = np.array([0.4, 0.3, 0.3])\n",
    "analytic_g = toy_objective_grad(alpha_test)\n",
    "fd_g = finite_difference_grad(lambda a: toy_objective(a), alpha_test)\n",
    "\n",
    "print(\"Testing toy objective gradient:\")\n",
    "print(f\"  Analytic gradient: {analytic_g}\")\n",
    "print(f\"  FD gradient:       {fd_g}\")\n",
    "print(f\"  Max difference:    {np.max(np.abs(fd_g - analytic_g)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_penalty_objective(alpha: np.ndarray, rho: float,\n",
    "                          X: np.ndarray = X_toy, p: np.ndarray = p_toy,\n",
    "                          gamma: float = gamma_toy) -> float:\n",
    "    \"\"\"\n",
    "    Quadratic penalty objective:\n",
    "        P(\u03b1, \u03c1) = f(\u03b1) + (\u03c1/2)(e^T \u03b1 - 1)^2\n",
    "    \"\"\"\n",
    "    f_val = toy_objective(alpha, X, p, gamma)\n",
    "    constraint_violation = np.sum(alpha) - 1.0\n",
    "    penalty = 0.5 * rho * constraint_violation ** 2\n",
    "    return f_val + penalty\n",
    "\n",
    "\n",
    "def toy_penalty_grad(alpha: np.ndarray, rho: float,\n",
    "                     X: np.ndarray = X_toy, p: np.ndarray = p_toy,\n",
    "                     gamma: float = gamma_toy) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient of quadratic penalty objective:\n",
    "        \u2207P(\u03b1, \u03c1) = \u2207f(\u03b1) + \u03c1(e^T \u03b1 - 1) * e\n",
    "    \"\"\"\n",
    "    grad_f = toy_objective_grad(alpha, X, p, gamma)\n",
    "    constraint_violation = np.sum(alpha) - 1.0\n",
    "    grad_penalty = rho * constraint_violation * np.ones_like(alpha)\n",
    "    return grad_f + grad_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test penalty gradient\n",
    "rho_test = 10.0\n",
    "analytic_gp = toy_penalty_grad(alpha_test, rho_test)\n",
    "fd_gp = finite_difference_grad(lambda a: toy_penalty_objective(a, rho_test), alpha_test)\n",
    "\n",
    "print(\"Testing penalty objective gradient:\")\n",
    "print(f\"  Analytic gradient: {analytic_gp}\")\n",
    "print(f\"  FD gradient:       {fd_gp}\")\n",
    "print(f\"  Max difference:    {np.max(np.abs(fd_gp - analytic_gp)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_12",
   "metadata": {},
   "source": [
    "## Section 3: Steepest Descent and Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    grad_f: Callable[[np.ndarray], np.ndarray],\n",
    "    x0: np.ndarray,\n",
    "    max_iter: int = 500,\n",
    "    tol: float = 1e-6,\n",
    "    alpha0: float = 1.0,\n",
    "    beta: float = 0.5,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, List[Tuple[int, float, float]]]:\n",
    "    \"\"\"\n",
    "    Steepest descent with backtracking line search.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function\n",
    "    grad_f : callable\n",
    "        Gradient function\n",
    "    x0 : np.ndarray\n",
    "        Initial point\n",
    "    max_iter : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Gradient norm tolerance for stopping\n",
    "    alpha0 : float\n",
    "        Initial step size for line search\n",
    "    beta : float\n",
    "        Backtracking factor\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple (x, history)\n",
    "        Final point and history of (iteration, f_val, grad_norm)\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "        fval = f(x)\n",
    "        history.append((k, fval, gnorm))\n",
    "        \n",
    "        if verbose and k % 50 == 0:\n",
    "            print(f\"  Iter {k}: f={fval:.6f}, ||\u2207f||={gnorm:.2e}\")\n",
    "        \n",
    "        if gnorm < tol:\n",
    "            if verbose:\n",
    "                print(f\"  Converged at iteration {k}\")\n",
    "            break\n",
    "        \n",
    "        # Descent direction\n",
    "        p = -g\n",
    "        \n",
    "        # Backtracking line search\n",
    "        alpha, f_new = backtracking_line_search(f, grad_f, x, p, alpha0, 1e-4, beta)\n",
    "        x = x + alpha * p\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(\n",
    "    f: Callable[[np.ndarray], float],\n",
    "    grad_f: Callable[[np.ndarray], np.ndarray],\n",
    "    hess_f: Callable[[np.ndarray], np.ndarray],\n",
    "    x0: np.ndarray,\n",
    "    max_iter: int = 100,\n",
    "    tol: float = 1e-6,\n",
    "    reg: float = 1e-8,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, List[Tuple[int, float, float]]]:\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and Hessian regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function\n",
    "    grad_f : callable\n",
    "        Gradient function\n",
    "    hess_f : callable\n",
    "        Hessian function\n",
    "    x0 : np.ndarray\n",
    "        Initial point\n",
    "    max_iter : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Gradient norm tolerance\n",
    "    reg : float\n",
    "        Regularization for near-singular Hessian\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple (x, history)\n",
    "        Final point and history\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    history = []\n",
    "    n = len(x)\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "        fval = f(x)\n",
    "        history.append((k, fval, gnorm))\n",
    "        \n",
    "        if verbose and k % 10 == 0:\n",
    "            print(f\"  Iter {k}: f={fval:.6f}, ||\u2207f||={gnorm:.2e}\")\n",
    "        \n",
    "        if gnorm < tol:\n",
    "            if verbose:\n",
    "                print(f\"  Converged at iteration {k}\")\n",
    "            break\n",
    "        \n",
    "        # Compute Hessian and regularize if needed\n",
    "        H = hess_f(x)\n",
    "        H_reg = H + reg * np.eye(n)\n",
    "        \n",
    "        # Newton direction: p = -H^{-1} g\n",
    "        try:\n",
    "            p = np.linalg.solve(H_reg, -g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback to gradient descent step\n",
    "            p = -g\n",
    "        \n",
    "        # Check descent direction; if not, use negative gradient\n",
    "        if g.dot(p) >= 0:\n",
    "            p = -g\n",
    "        \n",
    "        # Backtracking line search\n",
    "        alpha, f_new = backtracking_line_search(f, grad_f, x, p)\n",
    "        x = x + alpha * p\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_penalty_hessian(alpha: np.ndarray, rho: float,\n",
    "                        X: np.ndarray = X_toy, p: np.ndarray = p_toy,\n",
    "                        gamma: float = gamma_toy) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hessian of the penalty objective.\n",
    "    \n",
    "    For f(\u03b1) = -E[u(C)], the Hessian is:\n",
    "        \u2202\u00b2f/\u2202\u03b1_i\u2202\u03b1_k = -\u2211_j p_j u''(C_j) X_ij X_kj\n",
    "                     = -\u2211_j p_j (-\u03b3\u00b2) exp(-\u03b3 C_j) X_ij X_kj\n",
    "                     = \u03b3\u00b2 \u2211_j p_j exp(-\u03b3 C_j) X_ij X_kj\n",
    "    \n",
    "    For the penalty term (\u03c1/2)(e^T \u03b1 - 1)\u00b2, the Hessian is \u03c1 e e^T.\n",
    "    \"\"\"\n",
    "    m = len(alpha)\n",
    "    consumption = X.T @ alpha  # (n,)\n",
    "    u_double_prime = gamma**2 * np.exp(-gamma * consumption)  # (n,)\n",
    "    \n",
    "    # Hessian of f: H_f[i,k] = \u03b3\u00b2 \u2211_j p_j exp(-\u03b3 C_j) X_ij X_kj\n",
    "    # = X @ diag(p * \u03b3\u00b2 exp(-\u03b3C)) @ X^T\n",
    "    weights = p * u_double_prime\n",
    "    H_f = X @ np.diag(weights) @ X.T\n",
    "    \n",
    "    # Hessian of penalty: \u03c1 e e^T\n",
    "    e = np.ones(m)\n",
    "    H_penalty = rho * np.outer(e, e)\n",
    "    \n",
    "    return H_f + H_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Newton's method on the toy problem\n",
    "print(\"=\"*60)\n",
    "print(\"Testing steepest descent on toy problem\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rho = 100.0\n",
    "alpha0_sd = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "f_toy = lambda a: toy_penalty_objective(a, rho)\n",
    "grad_toy = lambda a: toy_penalty_grad(a, rho)\n",
    "hess_toy = lambda a: toy_penalty_hessian(a, rho)\n",
    "\n",
    "alpha_sd, hist_sd = steepest_descent(f_toy, grad_toy, alpha0_sd, max_iter=200, verbose=True)\n",
    "\n",
    "print(f\"\\nSteepest descent result:\")\n",
    "print(f\"  Final \u03b1: {alpha_sd}\")\n",
    "print(f\"  Sum of weights: {np.sum(alpha_sd):.6f} (should be \u2248 1)\")\n",
    "print(f\"  Final objective: {f_toy(alpha_sd):.6f}\")\n",
    "print(f\"  Constraint violation: {np.abs(np.sum(alpha_sd) - 1):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Testing Newton's method on toy problem\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "alpha_newton, hist_newton = newton_method(f_toy, grad_toy, hess_toy, alpha0_sd.copy(), \n",
    "                                          max_iter=50, verbose=True)\n",
    "\n",
    "print(f\"\\nNewton's method result:\")\n",
    "print(f\"  Final \u03b1: {alpha_newton}\")\n",
    "print(f\"  Sum of weights: {np.sum(alpha_newton):.6f} (should be \u2248 1)\")\n",
    "print(f\"  Final objective: {f_toy(alpha_newton):.6f}\")\n",
    "print(f\"  Constraint violation: {np.abs(np.sum(alpha_newton) - 1):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Objective value\n",
    "ax1 = axes[0]\n",
    "iters_sd, fvals_sd, _ = zip(*hist_sd)\n",
    "iters_n, fvals_n, _ = zip(*hist_newton)\n",
    "ax1.plot(iters_sd, fvals_sd, 'b-o', label='Steepest Descent', markersize=3)\n",
    "ax1.plot(iters_n, fvals_n, 'r-s', label='Newton', markersize=3)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Objective value')\n",
    "ax1.set_title('Convergence: Objective Value')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "ax2 = axes[1]\n",
    "_, _, gnorms_sd = zip(*hist_sd)\n",
    "_, _, gnorms_n = zip(*hist_newton)\n",
    "ax2.semilogy(iters_sd, gnorms_sd, 'b-o', label='Steepest Descent', markersize=3)\n",
    "ax2.semilogy(iters_n, gnorms_n, 'r-s', label='Newton', markersize=3)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Gradient norm')\n",
    "ax2.set_title('Convergence: Gradient Norm')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization/convergence_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Convergence comparison plot saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_19",
   "metadata": {},
   "source": [
    "## Section 4: S&P 500 Data: Read and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read S&P 500 data\n",
    "snp_path = 'optimization/snp500.txt'\n",
    "df_snp = pd.read_csv(snp_path)\n",
    "\n",
    "print(f\"S&P 500 data shape: {df_snp.shape}\")\n",
    "print(f\"Columns: Symbol, Name, Price, + {df_snp.shape[1] - 3} scenario columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_snp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data components\n",
    "symbols = df_snp['Symbol'].values\n",
    "names = df_snp['Name'].values\n",
    "prices = df_snp['Price'].values\n",
    "\n",
    "# Scenario returns are in columns 3 onwards (columns '1', '2', ..., '1000')\n",
    "scenario_cols = [str(i) for i in range(1, 1001)]\n",
    "R_raw = df_snp[scenario_cols].values  # shape: (n_assets, n_scenarios)\n",
    "\n",
    "print(f\"\\nNumber of assets: {len(symbols)}\")\n",
    "print(f\"Returns matrix R shape: {R_raw.shape} (assets \u00d7 scenarios)\")\n",
    "print(f\"\\nSample symbols: {symbols[:5]}\")\n",
    "print(f\"Sample prices: {prices[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are actually price scenarios, not returns. Compute returns as percentage change from initial price.\n",
    "# R_returns[i,j] = (R_raw[i,j] - prices[i]) / prices[i]\n",
    "R = (R_raw - prices[:, np.newaxis]) / prices[:, np.newaxis]\n",
    "\n",
    "print(\"Converting price scenarios to returns...\")\n",
    "print(f\"Returns matrix R shape: {R.shape}\")\n",
    "print(f\"\\nSample returns for first asset ({symbols[0]}):\")\n",
    "print(f\"  Mean return: {R[0].mean():.6f}\")\n",
    "print(f\"  Std return:  {R[0].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale returns: subtract per-asset mean, optionally divide by std\n",
    "R_mean = R.mean(axis=1, keepdims=True)\n",
    "R_std = R.std(axis=1, keepdims=True)\n",
    "\n",
    "# Centered returns (de-meaned)\n",
    "R_centered = R - R_mean\n",
    "\n",
    "# Standardized returns (z-scores)\n",
    "R_standardized = (R - R_mean) / (R_std + 1e-10)  # avoid division by zero\n",
    "\n",
    "print(\"Scaling statistics:\")\n",
    "print(f\"  Mean of means: {R_mean.mean():.6e}\")\n",
    "print(f\"  Mean of stds:  {R_std.mean():.6f}\")\n",
    "print(f\"\\nSample stats for first 5 assets:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {symbols[i]:6s}: mean={R[i].mean():.6f}, std={R[i].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_24",
   "metadata": {},
   "source": [
    "## Section 5: S&P 500 Portfolio Optimization via Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for speed: first 20 assets, first 100 scenarios\n",
    "n_assets_subset = 20\n",
    "n_scenarios_subset = 100\n",
    "\n",
    "R_subset = R[:n_assets_subset, :n_scenarios_subset]\n",
    "symbols_subset = symbols[:n_assets_subset]\n",
    "p_subset = np.ones(n_scenarios_subset) / n_scenarios_subset\n",
    "\n",
    "print(f\"Using subset: {n_assets_subset} assets \u00d7 {n_scenarios_subset} scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_objective(alpha: np.ndarray, R: np.ndarray, p: np.ndarray, \n",
    "                  gamma: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Negative expected utility for S&P portfolio.\n",
    "    Uses exponential utility.\n",
    "    \"\"\"\n",
    "    consumption = R.T @ alpha  # (n_scenarios,)\n",
    "    utilities = -np.exp(-gamma * consumption)\n",
    "    return -np.dot(p, utilities)\n",
    "\n",
    "\n",
    "def snp_objective_grad(alpha: np.ndarray, R: np.ndarray, p: np.ndarray,\n",
    "                       gamma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient of negative expected utility.\n",
    "    \"\"\"\n",
    "    consumption = R.T @ alpha  # (n_scenarios,)\n",
    "    u_prime = gamma * np.exp(-gamma * consumption)\n",
    "    return -R @ (p * u_prime)\n",
    "\n",
    "\n",
    "def snp_penalty_objective(alpha: np.ndarray, rho: float, R: np.ndarray,\n",
    "                          p: np.ndarray, gamma: float = 1.0) -> float:\n",
    "    \"\"\"Penalty objective with sum-to-one constraint.\"\"\"\n",
    "    f_val = snp_objective(alpha, R, p, gamma)\n",
    "    constraint = np.sum(alpha) - 1.0\n",
    "    return f_val + 0.5 * rho * constraint ** 2\n",
    "\n",
    "\n",
    "def snp_penalty_grad(alpha: np.ndarray, rho: float, R: np.ndarray,\n",
    "                     p: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Gradient of penalty objective.\"\"\"\n",
    "    grad_f = snp_objective_grad(alpha, R, p, gamma)\n",
    "    constraint = np.sum(alpha) - 1.0\n",
    "    return grad_f + rho * constraint * np.ones_like(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient on subset\n",
    "alpha_test_snp = np.ones(n_assets_subset) / n_assets_subset\n",
    "rho_snp = 100.0\n",
    "gamma_snp = 1.0\n",
    "\n",
    "analytic_g_snp = snp_penalty_grad(alpha_test_snp, rho_snp, R_subset, p_subset, gamma_snp)\n",
    "fd_g_snp = finite_difference_grad(\n",
    "    lambda a: snp_penalty_objective(a, rho_snp, R_subset, p_subset, gamma_snp),\n",
    "    alpha_test_snp\n",
    ")\n",
    "\n",
    "print(\"Testing S&P penalty gradient:\")\n",
    "print(f\"  Max difference: {np.max(np.abs(fd_g_snp - analytic_g_snp)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snp_optimization(init_name: str, alpha0: np.ndarray, \n",
    "                         R: np.ndarray, p: np.ndarray,\n",
    "                         rho: float = 1000.0, gamma: float = 1.0,\n",
    "                         max_iter: int = 300) -> Dict:\n",
    "    \"\"\"\n",
    "    Run steepest descent from given initialization.\n",
    "    \"\"\"\n",
    "    f = lambda a: snp_penalty_objective(a, rho, R, p, gamma)\n",
    "    grad = lambda a: snp_penalty_grad(a, rho, R, p, gamma)\n",
    "    \n",
    "    alpha_final, history = steepest_descent(f, grad, alpha0, max_iter=max_iter, \n",
    "                                           tol=1e-7, alpha0=0.1, beta=0.5)\n",
    "    \n",
    "    return {\n",
    "        'name': init_name,\n",
    "        'alpha': alpha_final,\n",
    "        'iterations': len(history),\n",
    "        'final_obj': f(alpha_final),\n",
    "        'grad_norm': np.linalg.norm(grad(alpha_final)),\n",
    "        'constraint_violation': np.abs(np.sum(alpha_final) - 1),\n",
    "        'history': history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple initializations\n",
    "n_assets = n_assets_subset\n",
    "rho_opt = 1000.0\n",
    "gamma_opt = 0.5\n",
    "\n",
    "# 1. Equal weights\n",
    "init_equal = np.ones(n_assets) / n_assets\n",
    "\n",
    "# 2. Random Dirichlet weights\n",
    "np.random.seed(123)\n",
    "init_dirichlet = np.random.dirichlet(np.ones(n_assets))\n",
    "\n",
    "# 3. Price-weighted portfolio\n",
    "prices_subset = prices[:n_assets_subset]\n",
    "init_price = prices_subset / np.sum(prices_subset)\n",
    "\n",
    "# 4. Concentrated (smoothed one-hot)\n",
    "init_concentrated = np.ones(n_assets) * 0.01\n",
    "init_concentrated[0] = 0.91  # Concentrate on first asset\n",
    "init_concentrated /= np.sum(init_concentrated)\n",
    "\n",
    "initializations = [\n",
    "    ('Equal weights', init_equal),\n",
    "    ('Dirichlet random', init_dirichlet),\n",
    "    ('Price-weighted', init_price),\n",
    "    ('Concentrated', init_concentrated)\n",
    "]\n",
    "\n",
    "print(\"Running S&P 500 portfolio optimization from multiple initializations...\")\n",
    "print(f\"Parameters: \u03c1={rho_opt}, \u03b3={gamma_opt}, max_iter=300\")\n",
    "print()\n",
    "\n",
    "results = []\n",
    "for name, alpha0 in initializations:\n",
    "    result = run_snp_optimization(name, alpha0, R_subset, p_subset, \n",
    "                                  rho=rho_opt, gamma=gamma_opt)\n",
    "    results.append(result)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Iterations: {result['iterations']}\")\n",
    "    print(f\"  Final obj:  {result['final_obj']:.6f}\")\n",
    "    print(f\"  Grad norm:  {result['grad_norm']:.2e}\")\n",
    "    print(f\"  Constraint: {result['constraint_violation']:.2e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final portfolios\n",
    "print(\"Final portfolio weights (first 5 assets):\")\n",
    "print(\"-\" * 60)\n",
    "header = \"Asset    \" + \"  \".join([r['name'][:10] for r in results])\n",
    "print(header)\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(5, n_assets)):\n",
    "    row = f\"{symbols_subset[i]:8s}\"\n",
    "    for r in results:\n",
    "        row += f\"  {r['alpha'][i]:8.4f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_31",
   "metadata": {},
   "source": [
    "## Section 6: Week-Only Return Model and Gauss\u2013Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-only return model: Assume returns can be modeled as a low-rank structure\n",
    "# R_ij \u2248 a_i * b_j (rank-1 approximation)\n",
    "#\n",
    "# This means each asset's return is proportional to a single \"market factor\" b_j\n",
    "# across scenarios, with asset-specific loading a_i.\n",
    "#\n",
    "# We fit this via nonlinear least squares:\n",
    "# min_{a, b} ||R - a b^T||_F^2\n",
    "#\n",
    "# This can be solved via Gauss-Newton or simply SVD.\n",
    "\n",
    "# Use small subset for demo\n",
    "n_assets_gn = 10\n",
    "n_scenarios_gn = 50\n",
    "R_gn = R[:n_assets_gn, :n_scenarios_gn]\n",
    "\n",
    "print(f\"Week-only model: fitting rank-1 approximation to R ({n_assets_gn}\u00d7{n_scenarios_gn})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank1_residual(params: np.ndarray, R: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Residual vector for rank-1 model: r = vec(R - a b^T)\n",
    "    params = [a; b] concatenated\n",
    "    \"\"\"\n",
    "    m, n = R.shape\n",
    "    a = params[:m]\n",
    "    b = params[m:]\n",
    "    R_approx = np.outer(a, b)\n",
    "    return (R - R_approx).ravel()\n",
    "\n",
    "\n",
    "def rank1_jacobian(params: np.ndarray, R: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Jacobian of residual w.r.t. params.\n",
    "    r_{ij} = R_{ij} - a_i b_j\n",
    "    \u2202r_{ij}/\u2202a_k = -b_j if k=i, 0 otherwise\n",
    "    \u2202r_{ij}/\u2202b_l = -a_i if l=j, 0 otherwise\n",
    "    \"\"\"\n",
    "    m, n = R.shape\n",
    "    a = params[:m]\n",
    "    b = params[m:]\n",
    "    \n",
    "    # Jacobian has shape (m*n, m+n)\n",
    "    J = np.zeros((m * n, m + n))\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            # \u2202r_{ij}/\u2202a_i = -b_j\n",
    "            J[idx, i] = -b[j]\n",
    "            # \u2202r_{ij}/\u2202b_j = -a_i\n",
    "            J[idx, m + j] = -a[i]\n",
    "            idx += 1\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "def gauss_newton(R: np.ndarray, max_iter: int = 50, tol: float = 1e-6,\n",
    "                 reg: float = 1e-6) -> Tuple[np.ndarray, np.ndarray, List]:\n",
    "    \"\"\"\n",
    "    Gauss-Newton for rank-1 model fitting.\n",
    "    \n",
    "    Returns (a, b, history) where history contains residual norms.\n",
    "    \"\"\"\n",
    "    m, n = R.shape\n",
    "    \n",
    "    # Initialize with SVD\n",
    "    U, S, Vt = np.linalg.svd(R, full_matrices=False)\n",
    "    a = U[:, 0] * np.sqrt(S[0])\n",
    "    b = Vt[0, :] * np.sqrt(S[0])\n",
    "    params = np.concatenate([a, b])\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        r = rank1_residual(params, R)\n",
    "        res_norm = np.linalg.norm(r)\n",
    "        history.append((k, res_norm))\n",
    "        \n",
    "        if res_norm < tol:\n",
    "            break\n",
    "        \n",
    "        J = rank1_jacobian(params, R)\n",
    "        \n",
    "        # Gauss-Newton step: (J^T J + reg*I)^{-1} J^T r\n",
    "        JTJ = J.T @ J\n",
    "        JTr = J.T @ r\n",
    "        \n",
    "        # Regularize\n",
    "        JTJ_reg = JTJ + reg * np.eye(len(params))\n",
    "        \n",
    "        try:\n",
    "            delta = np.linalg.solve(JTJ_reg, JTr)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        \n",
    "        # Update with damping if needed\n",
    "        alpha = 1.0\n",
    "        for _ in range(10):\n",
    "            params_new = params + alpha * delta\n",
    "            r_new = rank1_residual(params_new, R)\n",
    "            if np.linalg.norm(r_new) < res_norm:\n",
    "                params = params_new\n",
    "                break\n",
    "            alpha *= 0.5\n",
    "        else:\n",
    "            params = params + alpha * delta\n",
    "    \n",
    "    a_final = params[:m]\n",
    "    b_final = params[m:]\n",
    "    return a_final, b_final, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Gauss-Newton for week-only (rank-1) model...\")\n",
    "\n",
    "a_gn, b_gn, hist_gn = gauss_newton(R_gn, max_iter=30)\n",
    "\n",
    "print(f\"\\nGauss-Newton converged in {len(hist_gn)} iterations\")\n",
    "print(f\"Initial residual norm: {hist_gn[0][1]:.6f}\")\n",
    "print(f\"Final residual norm:   {hist_gn[-1][1]:.6f}\")\n",
    "\n",
    "# Compute approximation quality\n",
    "R_approx = np.outer(a_gn, b_gn)\n",
    "mse = np.mean((R_gn - R_approx) ** 2)\n",
    "r2 = 1 - np.sum((R_gn - R_approx) ** 2) / np.sum((R_gn - R_gn.mean()) ** 2)\n",
    "\n",
    "print(f\"\\nModel fit:\")\n",
    "print(f\"  MSE: {mse:.6e}\")\n",
    "print(f\"  R\u00b2:  {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with direct SVD solution\n",
    "U_svd, S_svd, Vt_svd = np.linalg.svd(R_gn, full_matrices=False)\n",
    "a_svd = U_svd[:, 0] * S_svd[0]\n",
    "b_svd = Vt_svd[0, :]\n",
    "R_svd = np.outer(a_svd, b_svd)\n",
    "\n",
    "mse_svd = np.mean((R_gn - R_svd) ** 2)\n",
    "r2_svd = 1 - np.sum((R_gn - R_svd) ** 2) / np.sum((R_gn - R_gn.mean()) ** 2)\n",
    "\n",
    "print(\"\\nComparison with SVD rank-1 approximation:\")\n",
    "print(f\"  SVD MSE: {mse_svd:.6e}\")\n",
    "print(f\"  SVD R\u00b2:  {r2_svd:.4f}\")\n",
    "print(f\"  GN MSE:  {mse:.6e}\")\n",
    "print(f\"  GN R\u00b2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the approximation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(R_gn, aspect='auto', cmap='RdBu_r')\n",
    "ax1.set_title('Original Returns R')\n",
    "ax1.set_xlabel('Scenario')\n",
    "ax1.set_ylabel('Asset')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(R_approx, aspect='auto', cmap='RdBu_r')\n",
    "ax2.set_title('Rank-1 Approximation (GN)')\n",
    "ax2.set_xlabel('Scenario')\n",
    "ax2.set_ylabel('Asset')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "ax3 = axes[2]\n",
    "im3 = ax3.imshow(R_gn - R_approx, aspect='auto', cmap='RdBu_r')\n",
    "ax3.set_title('Residual (R - Approx)')\n",
    "ax3.set_xlabel('Scenario')\n",
    "ax3.set_ylabel('Asset')\n",
    "plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization/model_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model comparison plot saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_37",
   "metadata": {},
   "source": [
    "## Section 7: Numerical Experiments and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NUMERICAL EXPERIMENTS AND DIAGNOSTICS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Toy problem with increasing penalty\n",
    "print(\"\\n--- Experiment 1: Toy problem with increasing penalty parameter ---\\n\")\n",
    "\n",
    "rho_values = [1, 10, 100, 1000, 10000]\n",
    "alpha0_exp = np.array([0.4, 0.3, 0.3])\n",
    "\n",
    "toy_results = []\n",
    "for rho in rho_values:\n",
    "    f = lambda a, r=rho: toy_penalty_objective(a, r)\n",
    "    grad = lambda a, r=rho: toy_penalty_grad(a, r)\n",
    "    \n",
    "    alpha_final, hist = steepest_descent(f, grad, alpha0_exp.copy(), max_iter=500, tol=1e-8)\n",
    "    \n",
    "    violation = np.abs(np.sum(alpha_final) - 1)\n",
    "    obj_val = toy_objective(alpha_final)\n",
    "    \n",
    "    toy_results.append({\n",
    "        'rho': rho,\n",
    "        'alpha': alpha_final,\n",
    "        'obj': obj_val,\n",
    "        'violation': violation,\n",
    "        'iters': len(hist)\n",
    "    })\n",
    "    \n",
    "    print(f\"\u03c1 = {rho:5d}: obj={obj_val:.6f}, violation={violation:.2e}, iters={len(hist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: S&P 500 with different subsets\n",
    "print(\"\\n--- Experiment 2: S&P 500 with different subset sizes ---\\n\")\n",
    "\n",
    "subset_sizes = [(10, 50), (15, 75), (20, 100)]\n",
    "snp_results = []\n",
    "\n",
    "for n_a, n_s in subset_sizes:\n",
    "    R_exp = R[:n_a, :n_s]\n",
    "    p_exp = np.ones(n_s) / n_s\n",
    "    alpha0_exp = np.ones(n_a) / n_a\n",
    "    \n",
    "    result = run_snp_optimization(f\"{n_a} assets, {n_s} scenarios\", \n",
    "                                  alpha0_exp, R_exp, p_exp,\n",
    "                                  rho=1000.0, gamma=0.5, max_iter=200)\n",
    "    snp_results.append(result)\n",
    "    \n",
    "    print(f\"{n_a:2d} assets, {n_s:3d} scenarios: \"\n",
    "          f\"obj={result['final_obj']:.4f}, \"\n",
    "          f\"violation={result['constraint_violation']:.2e}, \"\n",
    "          f\"iters={result['iterations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Week-only model on different subsets\n",
    "print(\"\\n--- Experiment 3: Week-only model on different subsets ---\\n\")\n",
    "\n",
    "model_sizes = [(5, 30), (10, 50), (15, 75)]\n",
    "model_results = []\n",
    "\n",
    "for n_a, n_s in model_sizes:\n",
    "    R_exp = R[:n_a, :n_s]\n",
    "    \n",
    "    a_exp, b_exp, hist_exp = gauss_newton(R_exp, max_iter=50)\n",
    "    \n",
    "    R_approx_exp = np.outer(a_exp, b_exp)\n",
    "    mse_exp = np.mean((R_exp - R_approx_exp) ** 2)\n",
    "    \n",
    "    # Also compute SVD for comparison\n",
    "    U_exp, S_exp, Vt_exp = np.linalg.svd(R_exp, full_matrices=False)\n",
    "    explained_var = S_exp[0]**2 / np.sum(S_exp**2)\n",
    "    \n",
    "    model_results.append({\n",
    "        'size': (n_a, n_s),\n",
    "        'mse': mse_exp,\n",
    "        'iters': len(hist_exp),\n",
    "        'explained_var': explained_var\n",
    "    })\n",
    "    \n",
    "    print(f\"{n_a:2d}\u00d7{n_s:2d}: MSE={mse_exp:.2e}, \"\n",
    "          f\"rank-1 explains {explained_var:.1%} variance, \"\n",
    "          f\"GN iters={len(hist_exp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTICS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for any issues\n",
    "issues = []\n",
    "\n",
    "# Check toy problem constraint satisfaction\n",
    "for r in toy_results:\n",
    "    if r['violation'] > 1e-4:\n",
    "        issues.append(f\"Toy (\u03c1={r['rho']}): constraint violation {r['violation']:.2e} > 1e-4\")\n",
    "\n",
    "# Check S&P optimization\n",
    "for r in snp_results:\n",
    "    if r['constraint_violation'] > 1e-4:\n",
    "        issues.append(f\"S&P ({r['name']}): constraint violation {r['constraint_violation']:.2e} > 1e-4\")\n",
    "    if r['grad_norm'] > 1e-4:\n",
    "        issues.append(f\"S&P ({r['name']}): gradient norm {r['grad_norm']:.2e} > 1e-4\")\n",
    "\n",
    "# Check for NaNs\n",
    "alpha_final_check = results[-1]['alpha']\n",
    "if np.any(np.isnan(alpha_final_check)):\n",
    "    issues.append(\"NaN detected in final portfolio weights\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nIssues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "    print(\"\\nRecommendation: Try smaller alpha0 or larger beta in line search.\")\n",
    "else:\n",
    "    print(\"\\nAll tests passed without issues!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Toy Problem Results (3 assets, 5 states):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Newton converged faster ({len(hist_newton)} iters) vs SD ({len(hist_sd)} iters)\")\n",
    "print(f\"  Final portfolio (Newton): {alpha_newton}\")\n",
    "print(f\"  Constraint violation: {np.abs(np.sum(alpha_newton) - 1):.2e}\")\n",
    "\n",
    "print(\"\\n2. S&P 500 Results (20 assets, 100 scenarios):\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"  {r['name']:20s}: {r['iterations']:3d} iters, \"\n",
    "          f\"violation={r['constraint_violation']:.2e}\")\n",
    "\n",
    "print(\"\\n3. Week-Only Model (Gauss-Newton):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  10\u00d750 subset: R\u00b2 = {r2:.4f}\")\n",
    "print(f\"  Gauss-Newton converged in {len(hist_gn)} iterations\")\n",
    "print(f\"  Model captures {r2:.1%} of variance with rank-1 approximation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_44",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrated portfolio optimization techniques for FINM 348:\n",
    "\n",
    "1. **Quadratic Penalty Method**: Successfully enforces equality constraints (sum-to-one) as \u03c1 increases\n",
    "\n",
    "2. **Steepest Descent vs Newton**: Newton's method shows faster convergence (quadratic vs linear), but requires Hessian computation\n",
    "\n",
    "3. **S&P 500 Optimization**: Different initializations converge to similar solutions, demonstrating robustness\n",
    "\n",
    "4. **Week-Only Model**: Gauss-Newton effectively fits a rank-1 approximation to return data, capturing significant variance with a simple factor model\n",
    "\n",
    "**Key Findings**:\n",
    "- For well-conditioned problems, Newton's method is preferred\n",
    "- Steepest descent is more robust to ill-conditioning\n",
    "- The rank-1 return model captures a substantial portion of variance, suggesting a strong market factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}