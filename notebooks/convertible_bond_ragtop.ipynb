{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8d9744",
   "metadata": {},
   "source": [
    "# Convertible Bond Pricing & Option-Market Calibration (ragtop reduced-form)\n",
    "\n",
    "This notebook prepares a report-ready scaffold for analysing a callable/convertible structure under a reduced-form jump-to-default framework. Heavy calibration grids and finite-difference (FD) sweeps are intentionally gated so that readers can rerun the expensive blocks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb7ad7",
   "metadata": {},
   "source": [
    "## 0. Setup & notebook controls\n",
    "\n",
    "We collect the required libraries, define runtime toggles, and verify the `ragtop` Black–Scholes API that we use throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0300d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import ragtop\n",
    "from ragtop.blackscholes import black_scholes\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 11\n",
    "\n",
    "RUN_HEAVY = False      # gate calibration loops and FD grids\n",
    "SAVE_ARTIFACTS = True\n",
    "VERBOSE = True\n",
    "\n",
    "ART_DIR = Path(\"../artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sanity check: ragtop Black–Scholes signature matches expectations.\n",
    "_test_px = black_scholes(-1, 625, 612, 0.05, 0.75, 0.62, borrow_cost=0.011)\n",
    "assert abs(_test_px - 113.34) < 1e-2, \"ragtop black_scholes sanity check failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfca875",
   "metadata": {},
   "source": [
    "## 1. Market data snapshot\n",
    "\n",
    "Risk-free rates are supplied as continuously-compounded spot rates (annualised). We interpolate linearly in maturity to recover $r(t)$, discount factors, and average rates used in the option and convertible pricing blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_df = pd.DataFrame(\n",
    "    [\n",
    "        (0.04988584, 0.004208395),\n",
    "        (0.12659817, 0.004846041),\n",
    "        (0.37591324, 0.007777790),\n",
    "        (0.62522831, 0.009878801),\n",
    "        (0.72111872, 0.010491200),\n",
    "        (1.71837900, 0.010167270),\n",
    "        (5.00000000, 0.020000000),\n",
    "    ],\n",
    "    columns=[\"time\", \"rate\"],\n",
    ")\n",
    "curve_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "_times = curve_df[\"time\"].to_numpy()\n",
    "_rates = curve_df[\"rate\"].to_numpy()\n",
    "_r_interp = interp1d(_times, _rates, kind=\"linear\", fill_value=\"extrapolate\", assume_sorted=True)\n",
    "\n",
    "\n",
    "def r_of(t: float) -> float:\n",
    "    \"\"\"Piecewise-linear spot rate r(t).\"\"\"\n",
    "    return float(_r_interp(np.clip(float(t), 0.0, float(_times[-1]))))\n",
    "\n",
    "\n",
    "def _integral_rate(t: float) -> float:\n",
    "    \"\"\"Integral of r from 0 to t using trapezoidal integration on the linear curve.\"\"\"\n",
    "    t = float(t)\n",
    "    if t <= 0.0:\n",
    "        return 0.0\n",
    "    knots = [0.0]\n",
    "    knots.extend(float(x) for x in _times if x < t)\n",
    "    if knots[-1] != t:\n",
    "        knots.append(t)\n",
    "    knots = np.asarray(knots)\n",
    "    rates = np.array([r_of(k) for k in knots])\n",
    "    return float(np.trapz(rates, knots))\n",
    "\n",
    "\n",
    "def discount_factor(t: float) -> float:\n",
    "    return float(np.exp(-_integral_rate(t)))\n",
    "\n",
    "\n",
    "def average_rate(t: float) -> float:\n",
    "    t = float(t)\n",
    "    return r_of(0.0) if t <= 0.0 else _integral_rate(t) / t\n",
    "\n",
    "\n",
    "# Discount monotonicity sanity check (cheap test kept active).\n",
    "assert discount_factor(2.0) < discount_factor(1.0) < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021baea6",
   "metadata": {},
   "source": [
    "### Equity, option quotes, and convertible terms\n",
    "\n",
    "All instruments reference the same underlying equity with spot $S_0 = 241.80$ and no dividends. Option maturities are aligned at $T_{\\mathrm{opt}} = 1.72$ years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ce73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 241.80\n",
    "DIVIDEND_YIELD = 0.0\n",
    "T_opt = 1.72\n",
    "\n",
    "option_quotes = pd.DataFrame(\n",
    "    [\n",
    "        (\"P230\", -1, 230.0, 52.40, 51.35, 53.45),\n",
    "        (\"P150\", -1, 150.0, 19.45, 17.90, 21.00),\n",
    "        (\"C370\", +1, 370.0, 13.95, 13.00, 14.90),\n",
    "    ],\n",
    "    columns=[\"label\", \"callput\", \"K\", \"mid\", \"bid\", \"ask\"],\n",
    ")\n",
    "\n",
    "NOTIONAL = 1_000.0\n",
    "CONVERSION_RATIO = 3.84615  # conversion price 260\n",
    "T_cb = 2.0\n",
    "RECOVERY_BOND = 0.40\n",
    "RECOVERY_EQUITY = 0.0\n",
    "BORROW_COST_OPTION = 0.0\n",
    "\n",
    "snapshot_rows = [\n",
    "    (\"Spot\", \"S0\", S0),\n",
    "    (\"Option\", \"T_opt\", T_opt),\n",
    "    (\"Convertible\", \"Notional\", NOTIONAL),\n",
    "    (\"Convertible\", \"Conversion ratio\", CONVERSION_RATIO),\n",
    "    (\"Convertible\", \"Tenor\", T_cb),\n",
    "    (\"Convertible\", \"Recovery (bond)\", RECOVERY_BOND),\n",
    "    (\"Convertible\", \"Recovery (equity)\", RECOVERY_EQUITY),\n",
    "]\n",
    "\n",
    "snapshot_table = pd.DataFrame(snapshot_rows, columns=[\"category\", \"field\", \"value\"])\n",
    "option_quotes, snapshot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b8fec",
   "metadata": {},
   "source": [
    "## 2. Part 1 — Constant-parameter pricing\n",
    "\n",
    "We splice a constant hazard level ($\\lambda = 7.5\\%$) into the equity via a \"dividend-like\" adjustment and use `ragtop.black_scholes` as the sole pricing and implied-volatility engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_price(callput: int, S: float, K: float, T: float, r: float, borrow_cost: float, sigma: float) -> float:\n",
    "    return float(black_scholes(callput, K, S, r, T, sigma, borrow_cost=borrow_cost))\n",
    "\n",
    "\n",
    "def implied_vol_with_ragtop(\n",
    "    callput: int,\n",
    "    S: float,\n",
    "    K: float,\n",
    "    T: float,\n",
    "    r: float,\n",
    "    price: float,\n",
    "    *,\n",
    "    borrow_cost: float = 0.0,\n",
    "    tol: float = 1e-8,\n",
    ") -> float:\n",
    "    \"\"\"Implied volatility via Brent's method on ragtop Black–Scholes.\"\"\"\n",
    "    if T <= 0 or price <= 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    def objective(sig: float) -> float:\n",
    "        return bs_price(callput, S, K, T, r, borrow_cost, sig) - price\n",
    "\n",
    "    low, high = 1e-4, 3.0\n",
    "    f_low, f_high = objective(low), objective(high)\n",
    "    if f_low * f_high > 0:\n",
    "        return float(\"nan\")\n",
    "    return float(brentq(objective, low, high, xtol=tol, rtol=tol))\n",
    "\n",
    "\n",
    "def default_intensity_fcn(t: float, S: float, b: float, p: float, Sref: float = S0) -> float:\n",
    "    S_safe = max(S, 1e-12)\n",
    "    return 0.075 * (b + (1.0 - b) * (Sref / S_safe) ** p)\n",
    "\n",
    "\n",
    "def hazard_lambda(S: np.ndarray, b: float, p: float, Sref: float = S0) -> np.ndarray:\n",
    "    S_arr = np.asarray(S, dtype=float)\n",
    "    safe = np.maximum(S_arr, 1e-12)\n",
    "    return 0.075 * (b + (1.0 - b) * (Sref / safe) ** p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_sigma = 0.50\n",
    "const_lambda = 0.075\n",
    "q_eff = DIVIDEND_YIELD + const_lambda\n",
    "r_bar_opt = average_rate(T_opt)\n",
    "\n",
    "rows = []\n",
    "for quote in option_quotes.itertuples(index=False):\n",
    "    bs_px = bs_price(quote.callput, S0, quote.K, T_opt, r_bar_opt, BORROW_COST_OPTION, const_sigma)\n",
    "    jtd_px = bs_price(quote.callput, S0, quote.K, T_opt, r_bar_opt, q_eff, const_sigma)\n",
    "    abs_err = jtd_px - quote.mid\n",
    "    rel_err = abs_err / quote.mid if quote.mid else float(\"nan\")\n",
    "    bs_iv = implied_vol_with_ragtop(quote.callput, S0, quote.K, T_opt, r_bar_opt, quote.mid, borrow_cost=BORROW_COST_OPTION)\n",
    "    jtd_iv = implied_vol_with_ragtop(quote.callput, S0, quote.K, T_opt, r_bar_opt, jtd_px, borrow_cost=BORROW_COST_OPTION)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"label\": quote.label,\n",
    "            \"cp\": quote.callput,\n",
    "            \"K\": quote.K,\n",
    "            \"mid\": quote.mid,\n",
    "            \"BS_px (q=0)\": bs_px,\n",
    "            \"JtD_px (q_eff=0.075)\": jtd_px,\n",
    "            \"abs_err\": abs_err,\n",
    "            \"rel_err\": rel_err,\n",
    "            \"BS_iv\": bs_iv,\n",
    "            \"JtD_iv\": jtd_iv,\n",
    "        }\n",
    "    )\n",
    "\n",
    "part1_table = pd.DataFrame(rows)\n",
    "\n",
    "# Put-call parity under effective dividend (synthetic put at strike 370).\n",
    "call_px = bs_price(+1, S0, 370.0, T_opt, r_bar_opt, q_eff, const_sigma)\n",
    "put_px = bs_price(-1, S0, 370.0, T_opt, r_bar_opt, q_eff, const_sigma)\n",
    "parity_lhs = call_px - put_px\n",
    "parity_rhs = S0 * math.exp(-q_eff * T_opt) - 370.0 * math.exp(-r_bar_opt * T_opt)\n",
    "assert abs(parity_lhs - parity_rhs) < 1e-8\n",
    "\n",
    "part1_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561584ed",
   "metadata": {},
   "source": [
    "## 3. Option-market calibration on the assignment grid\n",
    "\n",
    "We follow the specification exactly: volatilities $v \\in \\{0.20, 0.25, \\ldots, 0.90\\}$, hazard-shape exponents $p \\in \\{0, \\ldots, 8\\}$, and baseline weights $b \\in \\{1\\%, \\ldots, 10\\%\\}$. Each triple feeds the \"effective dividend\" shortcut $q_{\\mathrm{eff}} = \\lambda_0$ in the option pricer. The shortcut preserves put–call parity but does **not** solve the full jump-to-default SDE—it is only an intuitive mapping for this calibration exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca336fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_iv_rows = []\n",
    "for quote in option_quotes.itertuples(index=False):\n",
    "    iv_mid = implied_vol_with_ragtop(\n",
    "        quote.callput,\n",
    "        S0,\n",
    "        quote.K,\n",
    "        T_opt,\n",
    "        average_rate(T_opt),\n",
    "        quote.mid,\n",
    "        borrow_cost=BORROW_COST_OPTION,\n",
    "    )\n",
    "    market_iv_rows.append({\"label\": quote.label, \"callput\": quote.callput, \"K\": quote.K, \"mid\": quote.mid, \"iv_market\": iv_mid})\n",
    "\n",
    "market_iv_table = pd.DataFrame(market_iv_rows)\n",
    "market_iv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_grid_df = None\n",
    "best_calibration = None\n",
    "best_option_errors = None\n",
    "\n",
    "sigma_grid = np.arange(0.20, 0.901, 0.05)\n",
    "p_grid = np.arange(0, 9, 1)\n",
    "b_grid = np.arange(0.01, 0.101, 0.01)\n",
    "\n",
    "if RUN_HEAVY:\n",
    "    records = []\n",
    "    market_iv_lookup = {row.label: row.iv_market for row in market_iv_table.itertuples(index=False)}\n",
    "    rate_avg = average_rate(T_opt)\n",
    "    for sigma in sigma_grid:\n",
    "        for b in b_grid:\n",
    "            for p in p_grid:\n",
    "                lambda0 = default_intensity_fcn(0.0, S0, b, p)\n",
    "                borrow_eff = DIVIDEND_YIELD + lambda0\n",
    "                price_errors = []\n",
    "                iv_errors = []\n",
    "                for quote in option_quotes.itertuples(index=False):\n",
    "                    model_price = bs_price(quote.callput, S0, quote.K, T_opt, rate_avg, borrow_eff, sigma)\n",
    "                    price_errors.append(model_price - quote.mid)\n",
    "                    model_iv = implied_vol_with_ragtop(quote.callput, S0, quote.K, T_opt, rate_avg, model_price, borrow_cost=BORROW_COST_OPTION)\n",
    "                    market_iv = market_iv_lookup[quote.label]\n",
    "                    iv_errors.append(model_iv - market_iv)\n",
    "                rmse_price = float(np.sqrt(np.mean(np.square(price_errors))))\n",
    "                rmse_iv = float(np.sqrt(np.mean(np.square(iv_errors))))\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"sigma\": sigma,\n",
    "                        \"b\": b,\n",
    "                        \"p\": p,\n",
    "                        \"lambda0\": lambda0,\n",
    "                        \"rmse_price\": rmse_price,\n",
    "                        \"rmse_iv\": rmse_iv,\n",
    "                    }\n",
    "                )\n",
    "    calibration_grid_df = pd.DataFrame(records).sort_values(\"rmse_iv\").reset_index(drop=True)\n",
    "    best_calibration = calibration_grid_df.iloc[0].to_dict()\n",
    "\n",
    "    detail_rows = []\n",
    "    best_sigma = best_calibration[\"sigma\"]\n",
    "    best_b = best_calibration[\"b\"]\n",
    "    best_p = best_calibration[\"p\"]\n",
    "    lambda0 = best_calibration[\"lambda0\"]\n",
    "    borrow_eff = DIVIDEND_YIELD + lambda0\n",
    "    rate_avg = average_rate(T_opt)\n",
    "    for quote in option_quotes.itertuples(index=False):\n",
    "        model_price = bs_price(quote.callput, S0, quote.K, T_opt, rate_avg, borrow_eff, best_sigma)\n",
    "        error = model_price - quote.mid\n",
    "        model_iv = implied_vol_with_ragtop(quote.callput, S0, quote.K, T_opt, rate_avg, model_price, borrow_cost=BORROW_COST_OPTION)\n",
    "        detail_rows.append(\n",
    "            {\n",
    "                \"label\": quote.label,\n",
    "                \"model_price\": model_price,\n",
    "                \"mid\": quote.mid,\n",
    "                \"price_error\": error,\n",
    "                \"model_iv\": model_iv,\n",
    "                \"market_iv\": market_iv_lookup[quote.label],\n",
    "            }\n",
    "        )\n",
    "    best_option_errors = pd.DataFrame(detail_rows)\n",
    "\n",
    "    if SAVE_ARTIFACTS:\n",
    "        calibration_grid_df.to_csv(ART_DIR / \"cb_calibration_grid.csv\", index=False)\n",
    "        best_option_errors.to_csv(ART_DIR / \"cb_best_option_errors.csv\", index=False)\n",
    "else:\n",
    "    print(\"Calibration grid sweep is disabled. Set RUN_HEAVY=True to execute the full grid.\")\n",
    "\n",
    "calibration_grid_df if best_calibration else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_option_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ae03c",
   "metadata": {},
   "source": [
    "## 4. Convertible bond PDE solver\n",
    "\n",
    "We implement a Crank–Nicolson finite-difference engine with a Rannacher start, state-dependent intensity, and an early-conversion obstacle. The solver supports both PSOR and penalty formulations so we can verify the continuation region is genuine (rather than a numerical artefact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0600510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spot_grid(S_min: float, S_max: float, n_points: int, log_spacing: bool = True) -> np.ndarray:\n",
    "    S_min = max(S_min, 1e-6)\n",
    "    if log_spacing:\n",
    "        return np.exp(np.linspace(np.log(S_min), np.log(S_max), n_points))\n",
    "    return np.linspace(S_min, S_max, n_points)\n",
    "\n",
    "\n",
    "def build_operator(\n",
    "    S_grid: np.ndarray,\n",
    "    sigma: float,\n",
    "    r_val: float,\n",
    "    q_val: float,\n",
    "    hazard: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    n = len(S_grid)\n",
    "    lower = np.zeros(n)\n",
    "    diag = np.zeros(n)\n",
    "    upper = np.zeros(n)\n",
    "\n",
    "    alpha = 0.5 * sigma ** 2 * S_grid ** 2\n",
    "    beta = (r_val - q_val) * S_grid\n",
    "    gamma = -(r_val + hazard)\n",
    "\n",
    "    for i in range(1, n - 1):\n",
    "        d_down = S_grid[i] - S_grid[i - 1]\n",
    "        d_up = S_grid[i + 1] - S_grid[i]\n",
    "        second_lower = 2.0 / (d_down * (d_down + d_up))\n",
    "        second_upper = 2.0 / (d_up * (d_down + d_up))\n",
    "        second_diag = -second_lower - second_upper\n",
    "\n",
    "        first_lower = -d_up / (d_down * (d_down + d_up))\n",
    "        first_upper = d_down / (d_up * (d_down + d_up))\n",
    "        first_diag = -first_lower - first_upper\n",
    "\n",
    "        lower[i] = alpha[i] * second_lower + beta[i] * first_lower\n",
    "        diag[i] = alpha[i] * second_diag + beta[i] * first_diag + gamma[i]\n",
    "        upper[i] = alpha[i] * second_upper + beta[i] * first_upper\n",
    "\n",
    "    return lower, diag, upper\n",
    "\n",
    "\n",
    "def apply_operator(lower: np.ndarray, diag: np.ndarray, upper: np.ndarray, vec: np.ndarray) -> np.ndarray:\n",
    "    out = np.zeros_like(vec)\n",
    "    out[1:-1] = lower[1:-1] * vec[:-2] + diag[1:-1] * vec[1:-1] + upper[1:-1] * vec[2:]\n",
    "    return out\n",
    "\n",
    "\n",
    "def solve_tridiagonal(lower: np.ndarray, diag: np.ndarray, upper: np.ndarray, rhs: np.ndarray) -> np.ndarray:\n",
    "    n = len(rhs)\n",
    "    a = lower.copy()\n",
    "    b = diag.copy()\n",
    "    c = upper.copy()\n",
    "    d = rhs.copy()\n",
    "\n",
    "    for i in range(1, n):\n",
    "        if abs(b[i - 1]) < 1e-14:\n",
    "            continue\n",
    "        w = a[i] / b[i - 1]\n",
    "        b[i] -= w * c[i - 1]\n",
    "        d[i] -= w * d[i - 1]\n",
    "    x = np.zeros_like(rhs)\n",
    "    x[-1] = d[-1] / b[-1]\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        if abs(b[i]) < 1e-14:\n",
    "            x[i] = x[i + 1]\n",
    "        else:\n",
    "            x[i] = (d[i] - c[i] * x[i + 1]) / b[i]\n",
    "    return x\n",
    "\n",
    "\n",
    "def solve_convertible_cb(\n",
    "    sigma: float,\n",
    "    hazard_profile: Callable[[np.ndarray], np.ndarray],\n",
    "    r_fn: Callable[[float], float],\n",
    "    q_fn: Callable[[float], float],\n",
    "    T: float,\n",
    "    N: float,\n",
    "    CR: float,\n",
    "    R_b: float,\n",
    "    S0_eval: float,\n",
    "    *,\n",
    "    n_space: Optional[int] = None,\n",
    "    n_time: Optional[int] = None,\n",
    "    theta: float = 0.5,\n",
    "    log_spacing: bool = True,\n",
    "    method: str = \"psor\",\n",
    "    psor_omega: float = 1.4,\n",
    "    max_iter: int = 800,\n",
    "    tol: float = 1e-10,\n",
    "    allow_penalty_retry: bool = True,\n",
    ") -> Dict[str, object]:\n",
    "    n_space = int(n_space or (600 if RUN_HEAVY else 260))\n",
    "    n_time = int(n_time or (2400 if RUN_HEAVY else 900))\n",
    "\n",
    "    S_min = max(1e-3, S0_eval / 25.0)\n",
    "    S_max = 8.0 * max(S0_eval, 260.0)\n",
    "    S_grid = make_spot_grid(S_min, S_max, n_space, log_spacing=log_spacing)\n",
    "    payoff = np.maximum(N, CR * S_grid)\n",
    "    V = payoff.copy()\n",
    "    obstacle = CR * S_grid\n",
    "\n",
    "    dt = T / n_time\n",
    "    if dt <= 0:\n",
    "        raise ValueError(\"Non-positive timestep\")\n",
    "\n",
    "    steps = []\n",
    "    thetas = []\n",
    "    if theta != 0.5:\n",
    "        raise ValueError(\"theta other than 0.5 not supported in Rannacher schedule\")\n",
    "    steps.extend([dt / 2.0, dt / 2.0])\n",
    "    thetas.extend([1.0, 1.0])\n",
    "    steps.extend([dt] * (n_time - 1))\n",
    "    thetas.extend([theta] * (n_time - 1))\n",
    "\n",
    "    current_time = T\n",
    "    r_prev = r_fn(current_time)\n",
    "    q_prev = q_fn(current_time)\n",
    "    hazard_prev = hazard_profile(S_grid)\n",
    "    lower_prev, diag_prev, upper_prev = build_operator(S_grid, sigma, r_prev, q_prev, hazard_prev)\n",
    "\n",
    "    iter_log = []\n",
    "\n",
    "    for k, (dt_step, theta_step) in enumerate(zip(steps, thetas), start=1):\n",
    "        new_time = max(0.0, current_time - dt_step)\n",
    "        r_new = r_fn(new_time)\n",
    "        q_new = q_fn(new_time)\n",
    "        hazard_new = hazard_profile(S_grid)\n",
    "        lower_new, diag_new, upper_new = build_operator(S_grid, sigma, r_new, q_new, hazard_new)\n",
    "\n",
    "        A_l = -theta_step * dt_step * lower_new\n",
    "        A_d = 1.0 - theta_step * dt_step * diag_new\n",
    "        A_u = -theta_step * dt_step * upper_new\n",
    "\n",
    "        rhs = V.copy()\n",
    "        rhs += dt_step * ((1.0 - theta_step) * apply_operator(lower_prev, diag_prev, upper_prev, V))\n",
    "        source_term = (theta_step * hazard_new + (1.0 - theta_step) * hazard_prev) * R_b * N\n",
    "        rhs += dt_step * source_term\n",
    "\n",
    "        # Left boundary (bond ODE backward Euler)\n",
    "        lam_left = hazard_new[0]\n",
    "        rhs[0] = (V[0] + dt_step * lam_left * R_b * N) / (1.0 + dt_step * (r_new + lam_left))\n",
    "        A_l[0] = 0.0\n",
    "        A_u[0] = 0.0\n",
    "        A_d[0] = 1.0\n",
    "\n",
    "        # Right boundary (Neumann: V_S = CR)\n",
    "        dS_right = S_grid[-1] - S_grid[-2]\n",
    "        rhs[-1] = CR * dS_right\n",
    "        A_l[-1] = -1.0\n",
    "        A_d[-1] = 1.0\n",
    "        A_u[-1] = 0.0\n",
    "\n",
    "        if method.lower() == \"psor\":\n",
    "            V_new = V.copy()\n",
    "            residual = 1.0\n",
    "            sweep = 0\n",
    "            while sweep < max_iter and residual > tol:\n",
    "                residual = 0.0\n",
    "                for i in range(1, n_space - 1):\n",
    "                    rhs_i = rhs[i] - A_l[i] * V_new[i - 1] - A_u[i] * V_new[i + 1]\n",
    "                    x_i = (1.0 - psor_omega) * V_new[i] + (psor_omega / A_d[i]) * rhs_i\n",
    "                    projected = max(obstacle[i], x_i)\n",
    "                    residual = max(residual, abs(projected - V_new[i]))\n",
    "                    V_new[i] = projected\n",
    "                sweep += 1\n",
    "            if VERBOSE and (k <= 3 or k % 200 == 0):\n",
    "                iter_log.append((k, new_time, sweep, residual))\n",
    "            V = V_new\n",
    "        else:\n",
    "            penalty = 1e6\n",
    "            V_new = V.copy()\n",
    "            for sweep in range(min(6, max_iter)):\n",
    "                diag_mod = A_d.copy()\n",
    "                rhs_mod = rhs.copy()\n",
    "                active = V_new < obstacle\n",
    "                diag_mod[active] += penalty\n",
    "                rhs_mod[active] += penalty * obstacle[active]\n",
    "                V_new = solve_tridiagonal(A_l, diag_mod, A_u, rhs_mod)\n",
    "                V_new = np.maximum(V_new, obstacle)\n",
    "            V = V_new\n",
    "\n",
    "        current_time = new_time\n",
    "        r_prev, q_prev = r_new, q_new\n",
    "        hazard_prev = hazard_new\n",
    "        lower_prev, diag_prev, upper_prev = lower_new, diag_new, upper_new\n",
    "\n",
    "    price = float(np.interp(S0_eval, S_grid, V))\n",
    "    continuation_gap = V - obstacle\n",
    "    max_gap = float(np.max(continuation_gap))\n",
    "\n",
    "    if allow_penalty_retry and method.lower() == \"psor\" and max_gap <= 1e-4:\n",
    "        if VERBOSE:\n",
    "            print(\"PSOR solver produced no clear continuation region; retrying with penalty method.\")\n",
    "        return solve_convertible_cb(\n",
    "            sigma,\n",
    "            hazard_profile,\n",
    "            r_fn,\n",
    "            q_fn,\n",
    "            T,\n",
    "            N,\n",
    "            CR,\n",
    "            R_b,\n",
    "            S0_eval,\n",
    "            n_space=int(n_space * 1.25),\n",
    "            n_time=int(n_time * 1.25),\n",
    "            theta=theta,\n",
    "            log_spacing=log_spacing,\n",
    "            method=\"penalty\",\n",
    "            psor_omega=psor_omega,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            allow_penalty_retry=False,\n",
    "        )\n",
    "\n",
    "    positive_indices = np.where(continuation_gap > 1e-4)[0]\n",
    "\n",
    "    return {\n",
    "        \"price\": price,\n",
    "        \"S_grid\": S_grid,\n",
    "        \"values\": V,\n",
    "        \"obstacle\": obstacle,\n",
    "        \"continuation_gap\": continuation_gap,\n",
    "        \"positive_indices\": positive_indices,\n",
    "        \"log\": iter_log,\n",
    "        \"n_space\": n_space,\n",
    "        \"n_time\": n_time,\n",
    "        \"method\": method,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fa7f8",
   "metadata": {},
   "source": [
    "### Constant-parameter CB check (small grid)\n",
    "\n",
    "The following cell mirrors Part 1 by using $\\sigma=50\\%$ and $\\lambda=7.5\\%$ on a tiny grid. The computation is gated and intended to be rerun later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7981958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risky_bond_floor_constant(N: float, r: float, lam: float, T: float, R_b: float) -> float:\n",
    "    if r + lam == 0:\n",
    "        return N\n",
    "    term = math.exp(-(r + lam) * T)\n",
    "    return N * (R_b * lam / (r + lam) * (1.0 - term) + term)\n",
    "\n",
    "\n",
    "if RUN_HEAVY:\n",
    "    const_hazard_fn = lambda S: np.full_like(np.asarray(S, dtype=float), const_lambda)\n",
    "    const_cb = solve_convertible_cb(\n",
    "        sigma=const_sigma,\n",
    "        hazard_profile=const_hazard_fn,\n",
    "        r_fn=r_of,\n",
    "        q_fn=lambda _t: DIVIDEND_YIELD,\n",
    "        T=T_cb,\n",
    "        N=NOTIONAL,\n",
    "        CR=CONVERSION_RATIO,\n",
    "        R_b=RECOVERY_BOND,\n",
    "        S0_eval=S0,\n",
    "        n_space=200,\n",
    "        n_time=600,\n",
    "    )\n",
    "    r_bar_cb = average_rate(T_cb)\n",
    "    floor_val = risky_bond_floor_constant(NOTIONAL, r_bar_cb, const_lambda, T_cb, RECOVERY_BOND)\n",
    "    conversion = CONVERSION_RATIO * S0\n",
    "    assert floor_val <= const_cb[\"price\"] <= conversion + 1e-6\n",
    "    display({\n",
    "        \"CB_price\": const_cb[\"price\"],\n",
    "        \"conversion\": conversion,\n",
    "        \"risky_floor\": floor_val,\n",
    "        \"continuation_gap_max\": float(np.max(const_cb[\"continuation_gap\"])),\n",
    "    })\n",
    "else:\n",
    "    print(\"RUN_HEAVY=False → skipping the constant-parameter CB check (set RUN_HEAVY=True to execute).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00703953",
   "metadata": {},
   "source": [
    "## 5. Part 3 — Convertible pricing under calibrated and nearby parameters\n",
    "\n",
    "Once the option calibration grid is executed we reuse the best $(v,b,p)$ triple together with nearby perturbations to illustrate sensitivity. Each run checks for a non-trivial continuation region and enforces grid robustness when heavy mode is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_risky_floor(lambda_at_spot: float) -> float:\n",
    "    r_bar_cb = average_rate(T_cb)\n",
    "    lam = float(lambda_at_spot)\n",
    "    return risky_bond_floor_constant(NOTIONAL, r_bar_cb, lam, T_cb, RECOVERY_BOND)\n",
    "\n",
    "\n",
    "def cb_price_from_params(sigma: float, b: float, p: float, *, n_space: Optional[int] = None, n_time: Optional[int] = None) -> Dict[str, object]:\n",
    "    hazard_fn = lambda S: hazard_lambda(S, b, p)\n",
    "    result = solve_convertible_cb(\n",
    "        sigma=sigma,\n",
    "        hazard_profile=hazard_fn,\n",
    "        r_fn=r_of,\n",
    "        q_fn=lambda _t: DIVIDEND_YIELD,\n",
    "        T=T_cb,\n",
    "        N=NOTIONAL,\n",
    "        CR=CONVERSION_RATIO,\n",
    "        R_b=RECOVERY_BOND,\n",
    "        S0_eval=S0,\n",
    "        n_space=n_space,\n",
    "        n_time=n_time,\n",
    "        method=\"psor\",\n",
    "    )\n",
    "    if result[\"positive_indices\"].size == 0 and RUN_HEAVY:\n",
    "        if VERBOSE:\n",
    "            print(\"Upscaling grid because no continuation region was detected.\")\n",
    "        result = solve_convertible_cb(\n",
    "            sigma=sigma,\n",
    "            hazard_profile=hazard_fn,\n",
    "            r_fn=r_of,\n",
    "            q_fn=lambda _t: DIVIDEND_YIELD,\n",
    "            T=T_cb,\n",
    "            N=NOTIONAL,\n",
    "            CR=CONVERSION_RATIO,\n",
    "            R_b=RECOVERY_BOND,\n",
    "            S0_eval=S0,\n",
    "            n_space=int((n_space or (600 if RUN_HEAVY else 260)) * 1.4),\n",
    "            n_time=int((n_time or (2400 if RUN_HEAVY else 900)) * 1.4),\n",
    "            method=\"penalty\",\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "if RUN_HEAVY and best_calibration is not None:\n",
    "    base_sigma = best_calibration[\"sigma\"]\n",
    "    base_b = best_calibration[\"b\"]\n",
    "    base_p = best_calibration[\"p\"]\n",
    "    base_lambda = default_intensity_fcn(0.0, S0, base_b, base_p)\n",
    "\n",
    "    scenarios = [\n",
    "        (\"calibrated\", base_sigma, base_b, base_p),\n",
    "        (\"low_sigma\", max(0.20, base_sigma - 0.05), base_b, max(0, base_p - 1)),\n",
    "        (\"high_sigma\", min(0.90, base_sigma + 0.05), min(0.10, base_b + 0.02), min(8, base_p + 1)),\n",
    "        (\"higher_b\", base_sigma, min(0.10, base_b + 0.02), base_p),\n",
    "    ]\n",
    "\n",
    "    cb_rows = []\n",
    "    for tag, sigma, b, p in scenarios:\n",
    "        result = cb_price_from_params(sigma, b, p)\n",
    "        lambda_spot = default_intensity_fcn(0.0, S0, b, p)\n",
    "        floor_val = approximate_risky_floor(lambda_spot)\n",
    "        conversion_val = CONVERSION_RATIO * S0\n",
    "        gap_max = float(np.max(result[\"continuation_gap\"]))\n",
    "        assert gap_max > 1e-4 or abs(result[\"price\"] - conversion_val) < 1e-3\n",
    "        cb_rows.append(\n",
    "            {\n",
    "                \"tag\": tag,\n",
    "                \"sigma\": sigma,\n",
    "                \"b\": b,\n",
    "                \"p\": p,\n",
    "                \"lambda(S0)\": lambda_spot,\n",
    "                \"CB_PV\": result[\"price\"],\n",
    "                \"Conv (CR*S0)\": conversion_val,\n",
    "                \"Floor (approx)\": floor_val,\n",
    "                \"max_gap\": gap_max,\n",
    "                \"grid\": (result[\"n_space\"], result[\"n_time\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if RUN_HEAVY:\n",
    "            refined = cb_price_from_params(sigma, b, p, n_space=int(result[\"n_space\"] * 1.1), n_time=int(result[\"n_time\"] * 1.1))\n",
    "            diff = abs(refined[\"price\"] - result[\"price\"])\n",
    "            reference = max(1.0, refined[\"price\"])\n",
    "            assert diff / reference < 5e-4, \"Grid robustness check failed (>5 bps).\"\n",
    "\n",
    "        positive_nodes = result[\"positive_indices\"]\n",
    "        if VERBOSE:\n",
    "            if positive_nodes.size:\n",
    "                idx_sample = positive_nodes[:5]\n",
    "                s_vals = result[\"S_grid\"][idx_sample]\n",
    "                gaps = result[\"continuation_gap\"][idx_sample]\n",
    "                print(f\"{tag}: continuation nodes (sample) -> {list(zip(np.round(s_vals,2), np.round(gaps,4)))}\")\n",
    "            else:\n",
    "                print(f\"{tag}: continuation region touches conversion boundary (gap max={gap_max:.2e}).\")\n",
    "\n",
    "    cb_results_df = pd.DataFrame(cb_rows)\n",
    "    if SAVE_ARTIFACTS:\n",
    "        cb_results_df.to_csv(ART_DIR / \"cb_pricing_scenarios.csv\", index=False)\n",
    "    cb_results_df\n",
    "elif RUN_HEAVY:\n",
    "    print(\"Calibration results unavailable; run the calibration grid first.\")\n",
    "else:\n",
    "    print(\"RUN_HEAVY=False → convertible PDE scenarios are prepared but not executed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13a41a",
   "metadata": {},
   "source": [
    "## 6. Part 4 — Convertible value versus spot (preview loop)\n",
    "\n",
    "A spot sweep illustrates the floor, continuation, and early-conversion regimes. The code is provided but left unexecuted by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HEAVY and best_calibration is not None:\n",
    "    spots = np.linspace(20.0, 500.0, 25)\n",
    "    values = []\n",
    "    for s in spots:\n",
    "        hazard_fn = lambda S: hazard_lambda(S, best_calibration[\"b\"], best_calibration[\"p\"])\n",
    "        result = solve_convertible_cb(\n",
    "            sigma=best_calibration[\"sigma\"],\n",
    "            hazard_profile=hazard_fn,\n",
    "            r_fn=r_of,\n",
    "            q_fn=lambda _t: DIVIDEND_YIELD,\n",
    "            T=T_cb,\n",
    "            N=NOTIONAL,\n",
    "            CR=CONVERSION_RATIO,\n",
    "            R_b=RECOVERY_BOND,\n",
    "            S0_eval=s,\n",
    "            n_space=320,\n",
    "            n_time=900,\n",
    "        )\n",
    "        lam_spot = default_intensity_fcn(0.0, s, best_calibration[\"b\"], best_calibration[\"p\"])\n",
    "        floor_val = approximate_risky_floor(lam_spot)\n",
    "        values.append(\n",
    "            {\n",
    "                \"S\": s,\n",
    "                \"CB\": result[\"price\"],\n",
    "                \"Conversion\": CONVERSION_RATIO * s,\n",
    "                \"Floor\": floor_val,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sweep_df = pd.DataFrame(values)\n",
    "    if SAVE_ARTIFACTS:\n",
    "        sweep_df.to_csv(ART_DIR / \"cb_spot_sweep.csv\", index=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(sweep_df[\"S\"], sweep_df[\"CB\"], label=\"Convertible\")\n",
    "    ax.plot(sweep_df[\"S\"], sweep_df[\"Conversion\"], label=\"Conversion value\", linestyle=\"--\")\n",
    "    ax.plot(sweep_df[\"S\"], sweep_df[\"Floor\"], label=\"Risky floor\", linestyle=\":\")\n",
    "    ax.set_xlabel(\"Spot level\")\n",
    "    ax.set_ylabel(\"Present value\")\n",
    "    ax.set_title(\"Convertible value vs spot (preview grid)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "else:\n",
    "    print(\"RUN_HEAVY=False → spot sweep is disabled. Enable heavy mode for the full plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02812dcc",
   "metadata": {},
   "source": [
    "## 7. Appendix — Controls & reproducibility\n",
    "\n",
    "* Toggle `RUN_HEAVY` to `True` before executing the calibration grid or the PDE sweeps.\n",
    "* The FD solver defaults to log-spaced spot grids, Rannacher start, PSOR enforcement, and automatically retries with the penalty formulation if no continuation region appears.\n",
    "* All optional CSV exports land in `../artifacts/` when `SAVE_ARTIFACTS` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_HEAVY:\n",
    "    print(\"Heavy runs are enabled — expect longer runtimes for calibration and FD pricing.\")\n",
    "else:\n",
    "    print(\"Heavy runs are disabled. Set RUN_HEAVY=True to execute calibration grids and PDE sweeps.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
